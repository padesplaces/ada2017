{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT - *My Way* of seeing music covers\n",
    "#### Pierre-Antoine Desplaces, Ana√Øs Ladoy, Lou Richard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from io import StringIO\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook plan\n",
    "1. Data importation\n",
    "2. Clique organisation (Multi-level indexing)\n",
    "3. Addition of the language and the year of each track (SHS website web-scraping)\n",
    "4. Addition of the tempo and song hotness of each track (Access to track files through the cluster)\n",
    "5. Determine artist location for spatial analysis\n",
    "6. Addition of the genre for each track (Use of LastFM dataset and external website for genre listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Data importation\n",
    "Download available additional files containing metadata about our dataset from the cluster (dataset/million-songs_untar/)\n",
    "- tracks_per_year.txt\n",
    "- unique_tracks.txt\n",
    "- unique_artists.txt\n",
    "- artist_location.txt\n",
    "\n",
    "Use the Second Hand Songs (SHS) dataset that was created through a collaboration between the Million Songs team and the Second Hand Songs website (https://secondhandsongs.com/). These data are splitted into two datasets to allowed machine learnings algorithms (a train and a test set).\n",
    "- SHS_testset.txt\n",
    "- SHS_trainset.txt\n",
    "\n",
    "The use of external dataset (LastFM) for the genres and the use of the track files (.h5) available through the cluster are commented in part 4 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the additional files were downloaded from the cluster giving all the metadata of the Million Songs dataset. They will help to elaborate a plan and a script will then search more information about a specific track (h5 files in the cluster) maybe using cluster cpu. The path to access to a track in the cluster is for example million-songs/data/A/A/A (with the 3 letters at the end being the 3rd, 4th and 5th letter on the track id).\n",
    "- The music covers will be detected using another dataset (SecondHandSongs), we have the choice to use the downloadable dataset containing 18,196 tracks (all with a connection to the MSD dataset), or to web-scrapp the SHS website (https://secondhandsongs.com/) where we have much more information (522 436 covers) but not necessarly connected to our MSD dataset. The SHS API is RESTful (return a JSON object) and we are limited to 100 requests per minute and 1000 requestion per hour but we can contact them to remove limitation.\n",
    "- Some artist are geolocalised (30% of the MSD total artists) on the artist_location dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Additional files\n",
    "tracks_per_year=pd.read_csv('data/AdditionalFiles/tracks_per_year.txt',delimiter='<SEP>',engine='python',header=None,index_col=1,names=['year','trackID','artist','title'])\n",
    "unique_tracks=pd.read_csv('data/AdditionalFiles/unique_tracks.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['trackID','songID','artist','title'])\n",
    "unique_artists=pd.read_csv('data/AdditionalFiles/unique_artists.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['artistID','artistMID','randomTrack','name'])\n",
    "artist_location=pd.read_csv('data/AdditionalFiles/artist_location.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['artistID','lat','long','name','location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if indexes is unique and print the number of elements for each dataframe\n",
    "print('Dataframe (Unique index, Number of elements)')\n",
    "print('tracks_per_year ',(tracks_per_year.index.is_unique,tracks_per_year.shape[0]))\n",
    "print('unique_tracks ',(unique_tracks.index.is_unique,unique_tracks.shape[0]))\n",
    "print('unique_artists ',(unique_artists.index.is_unique,unique_artists.shape[0]))\n",
    "print('artist_location ',(artist_location.index.is_unique,artist_location.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covers dataset (SHS_testset.txt and SHS_trainset.txt) were organised in a very special way where group (named \"cliques\") list some tracks that are interrelated (music covers and original track). The function **read_shs_files** is used to import the files keeping the \"clique\" configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_shs_files(pathToFile):\n",
    "    f = open(pathToFile)\n",
    "    s = StringIO()\n",
    "    cur_ID = None\n",
    "    for ln in f:\n",
    "        if not ln.strip():\n",
    "                continue\n",
    "        if ln.startswith('%'):\n",
    "                cur_ID = ln.replace('\\n','<SEP>',1)\n",
    "                continue\n",
    "        if cur_ID is None:\n",
    "                print ('NO ID found')\n",
    "                sys.exit(1)\n",
    "        s.write(cur_ID + ln)\n",
    "    s.seek(0)\n",
    "    df = pd.read_csv(s,delimiter='<SEP>',engine='python',header=None,names=['shsID','trackID','artistID','shsPerf'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import the two SHS datasets and concatenate them\n",
    "SHS_testset=read_shs_files('data/SHS_testset.txt')\n",
    "SHS_trainset=read_shs_files('data/SHS_trainset.txt')\n",
    "covers=pd.concat([SHS_testset,SHS_trainset])\n",
    "covers.shsID=covers.shsID.str.strip('%')\n",
    "covers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clique organisation (Multi-level indexing)\n",
    "As said before, the cover dataset is organised as cliques that group some specific tracks that are interrelated. \n",
    "In order to keep this structure, we decide to use a multilevel index with cliques (need to transform shsID category in int) and then use the ranking according the released date of the track (year attribute) for the second index (thus, 0 will be the original song)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert shsID to clique id (first convert to category and get a code)\n",
    "covers=covers.assign(clique_id=(covers.shsID.astype('category')).cat.codes)\n",
    "#Remove the shsID column (useless since we have the clique_id now)\n",
    "covers.drop('shsID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the informations contained in the metadata files first, we merged some necessary attributes (name of the artist, title of the track, released date) from the MSD dataframes named before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge with unique_artists dataframe to find the artist name for each track (no taking consideration of featuring since we take only the name of the artist assigned with the track)\n",
    "covers=covers.merge(unique_artists[['name']],how='left',left_on='artistID',right_index=True)\n",
    "#Merge with unique_tracks dataframe to find the track name\n",
    "covers=covers.merge(unique_tracks[['title']],how='left',left_on='trackID',right_index=True)\n",
    "#Merge with tracks_per_year dataframe to find the year of each track\n",
    "covers=covers.merge(tracks_per_year[['year']],how='left',left_on='trackID',right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is printed some useful informations about the cover dataset (the basis of our work) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of tracks :', covers.shape[0])\n",
    "print('Number of cliques :', max(covers.index)+1) #Number of cliques (+1 because id starts at 0)\n",
    "print('Number of unique tracks :', len(covers.trackID.unique())) \n",
    "print('Number of unique artists :', len(covers.artistID.unique()))\n",
    "print('Number of missing trackID :', len(covers[covers.trackID.isnull()]))\n",
    "print('Number of missing artistID :', len(covers[covers.artistID.isnull()]))\n",
    "print('Number of missing years :', len(covers[covers.year.isnull()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covers=covers.sort_values(['clique_id', 'year'], ascending=[True, True]).reset_index() #Reset index according clique_id and year\n",
    "covers.drop('index',axis=1,inplace=True) #Drop the previous index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Addition of the language and the year of each track (SHS website web-scraping)\n",
    "With the results printed above, we noticed that 4796 years were missing (26%) and since we thought to detect the original song by ranking the tracks year in each clique, we needed to find a way to get them.\n",
    "Furthermore, year isn't necessarly sufficient informations to discriminate the tracks (cover appears sometimes in the same year than the original one), thus it will be better to have the released date for ALL the tracks if the information is available in the SHS website.\n",
    "\n",
    "Indeed, the Second Hand Songs website allows API request on their database (limited to 1000 requests per hour).\n",
    "Each track has a performance page where we can have access additional informations about the track as the language, the released date and the original song of the specific cover. In the SHS website, the performance id (that is used in the URL to access to the performance page) is available in our cover dataframe (shsPerf).\n",
    "Nevertheless, we have negative values of shsPerf and it corresponds to missing values.\n",
    "\n",
    "Thus, we have two ways to access extract the language/year/original song via web-scrapping :\n",
    "- For valid SHS performance ID, access to the performance page (e.g. 'https://secondhandsongs.com/performance/1983') and web-scrapping of the Language and Released date informations using the perfInfo() function.\n",
    "- For invalid SHS performance ID, API request to the search page (e.g. 'https://secondhandsongs.com/search/performance?title=blackbird&performer=beatles'), extract the perf ID with the find_PerfID() and then use the perfInfo() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of missing years with valid shsPerf (API request on the performance page) :',len(covers[(covers.year.isnull()) & (covers.shsPerf != -1)]))\n",
    "print('Number of missing years with invalid shsPerf (API request on the search page to find shsPerf) :',len(covers[(covers.year.isnull())])-len(covers[(covers.year.isnull()) & (covers.shsPerf != -1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(covers,open('covers.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test these algorithms, we have worked for now with a part of the cover dataframe (part dataframe), containing only 962 tracks but being representative of the covers dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge with the unique_tracks dataframe to get the name of the artist for the track (take featuring as well), it will be useful for the find_shsPerf function \n",
    "covers=covers.merge(unique_tracks[['artist']],how='left',left_on='trackID',right_index=True)\n",
    "covers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of cliques in the subset :', len(covers.clique_id.unique()))\n",
    "print('Number of tracks in the subset :', covers.shape[0])\n",
    "print('Number of missing years in the subset :', len(covers[covers.year.isnull()]))\n",
    "print('Number of invalid shsPerf in the subset :', len(covers[covers.shsPerf<0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#API request to find the SHS perf for the unvalid ones (negative values)\n",
    "def find_shsPerf(x):\n",
    "    title=part.iloc[x]['title']\n",
    "    artist=part.iloc[x]['artist']\n",
    "    shsPerf=part.iloc[x]['shsPerf']\n",
    "    \n",
    "    if shsPerf<0:\n",
    "        title=title.replace('.', '').replace('_', '').replace('/', '').lower().replace(' ','+')\n",
    "        artist=artist.replace('.', '').replace('_', '').replace('/', '').lower().replace(' ','+')\n",
    "        r=requests.get('https://secondhandsongs.com/search/performance?title='+title+'&op_title=contains&performer='+artist+'&op_performer=contains')\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        results=soup.find('tbody')\n",
    "\n",
    "        if results is None :\n",
    "            new_shsPerf=0\n",
    "        else:\n",
    "            new_shsPerf=int(results.find('a',attrs={'class':'link-performance'})['href'].split('/')[2])\n",
    "    else :\n",
    "        new_shsPerf=shsPerf\n",
    "        \n",
    "    return new_shsPerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the shsPerf for the tracks which doesn't have valid ones (substract 2055 to part dataframe index to start with index=0)\n",
    "#covers_withSHS=covers.shsPerf.index.map(lambda x: find_shsPerf(x-18196)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#covers_withSHS=pd.concat([part1, part2, part3, part4, part5, part6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(covers_withSHS,open('data/covers_withSHS.p','wb'))\n",
    "covers_withSHS=pickle.load(open(\"data/covers_withSHS.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covers_withSHS[covers_withSHS.shsPerf==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covers_part=covers_withSHS[10000:18196]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still can't access the SHS pages for **1050** music covers. We will need to decide if remove them since it will be impossible to find the missing release date and/or the language of the track.\n",
    "For now, we will compute the perfInfo_SHS for all the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#API request to SHS website for the page of a specific performance (defined as shsPerf) to extract Language and Date\n",
    "def perfInfo_SHS(shsPerf):\n",
    "    if shsPerf==0:\n",
    "        perfLanguage='Unavailable'\n",
    "        perfDate='Unavailable'\n",
    "        original_shsPerf='Unavailable'\n",
    "        \n",
    "    else :\n",
    "        r = requests.get('https://secondhandsongs.com/performance/'+str(shsPerf))\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        perfMeta=soup.find('dl',attrs={'class':'dl-horizontal'})\n",
    "        if perfMeta is None:\n",
    "            perfLanguage='Missing'\n",
    "            perfDate='Missing'\n",
    "            original_shsPerf='Missing'\n",
    "        else :\n",
    "            perfLanguage=perfMeta.find('dd',attrs={'itemprop':'inLanguage'})\n",
    "            if perfLanguage is None :\n",
    "                perfLanguage='Missing'\n",
    "            else :\n",
    "                perfLanguage=perfLanguage.text\n",
    "\n",
    "            perfDate=perfMeta.find('div',attrs={'class':'media-body'})\n",
    "            if perfDate is None :\n",
    "                perfDate='Missing'\n",
    "            else :\n",
    "                perfDate=perfDate.find('p').text.split('\\n')[2].strip(' ')\n",
    "\n",
    "            original_shsPerf=soup.find('section',attrs={'class':'work-originals'})\n",
    "            if original_shsPerf is None :\n",
    "                original_shsPerf='Missing'\n",
    "            else :\n",
    "                original_shsPerf=original_shsPerf.find('a',attrs={'class':'link-work'})['href'].split('/')[2]\n",
    "\n",
    "    return perfLanguage,perfDate,original_shsPerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#covers_part['language'], \\\n",
    "#covers_part['date'], \\\n",
    "#covers_part['original_shsPerf']= zip(*covers_part.shsPerf.map(perfInfo_SHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(covers_part,open('data/covers_10000_18196.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covers_part1 = pickle.load(open(\"data/covers_0_10000.p\",\"rb\"))\n",
    "covers_part2 = pickle.load(open(\"data/covers_10000_18196.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covers=pd.concat([covers_part1, covers_part2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed several problems with this intermediate result :\n",
    "\n",
    "1/ The track year (year) is sometimes different form the released date (date) we've extracted from the SHS website, we will prefer the data information found in the SHS website.\n",
    "\n",
    "2/ Some original performance that were found in the SHS website don't appear in the clique so need to add these tracks to our dataframe.\n",
    "- See for each clique if there is at least one original song defined\n",
    "- See if the original song is unique (in each clique)\n",
    "- Check if we have information about the orginal song in our dataframe or if we need to web-scrap from SHS website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#At least one original song in each clique ?\n",
    "#Replace missing values by Nan in the original_shsPerf column (to don't count them as unique values)\n",
    "covers.original_shsPerf.replace(['Missing','Unavailable'],np.NaN,inplace=True)\n",
    "count_unique=covers.groupby('clique_id').original_shsPerf.nunique(dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is the original song unique in each clique ?\n",
    "len(count_unique[count_unique>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covers[covers.clique_id==271]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of cliques with no original song found\n",
    "len(count_unique[count_unique==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "shsPerfUnique=pd.DataFrame(covers.shsPerf.unique())\n",
    "originalPerfUnique=pd.DataFrame(covers.originalPerfUnique.unique())\n",
    "merge=originalPerfUnique.merge(shsPerfUnique,how='left')\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the information we obtained for now. We noticed several problems that needs to be fixed later :\n",
    "- The track year (year) is sometimes different form the released date (date) we've extracted from the SHS website, we will prefer the data information found in the SHS website.\n",
    "- Some languages and some dates are missing (we will consider droping these covers or restrict our analyse concerning these parameters to only a subset of cliques).\n",
    "- Some original performance that were found in the SHS website don't appear in the clique so need to add these tracks to our dataframe (find them in the cluster or webscrap directly to the SHS website)\n",
    "- We may still have a problem to find the ranking (just discriminate the original track and do not sort the music covers).\n",
    "\n",
    "The work for this part will be to extend the analysis to the entire cover dataframe, resolve problems cited above and finish the multilevel indexing using ranking according the date in each clique. The API request is limited for the Second Hand Songs (SHS) website to 1000 requests per hour. Due to the large number of requests needed (668 to resolve the missing SHS problem and 18196 to find the Language/Year/Original Song), we'll maybe ask to the SHS team an exception to remove this limitation.\n",
    "\n",
    "Then, the goal is to add to this dataframe the informations we've extracted through the cluster (tempo, song hotness), through the LastFM dataset (genre) and through the artist_location table (country of the artist) with the methods we describe in the next section. \n",
    "\n",
    "Thus, we'll have all the informations required to start the analysis of our music covers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Take the clique id we defined as id of the dataframe (not unique index for now)\n",
    "#covers.set_index('clique_id',inplace=True)\n",
    "#covers.sort_index(inplace=True)\n",
    "\n",
    "#Compute the order of songs for each clique\n",
    "#covers['rank']=covers.groupby('clique_id')['year'].rank(method='dense',ascending=True).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Access to files (tempo / dancability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We open the first file of the subset, to check what the HDF5 keys are and then we read each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(\"data/MillionSongSubset/data/A/A/A/TRAAAAW128F429D538.h5\") as hdf:\n",
    "    print(hdf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_hdf(\"data/MillionSongSubset/data/A/A/A/TRAAAAW128F429D538.h5\",\"/analysis/songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_hdf(\"data/MillionSongSubset/data/A/A/A/TRAAAAW128F429D538.h5\",\"/metadata/songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_hdf(\"data/MillionSongSubset/data/A/A/A/TRAAAAW128F429D538.h5\",\"/musicbrainz/songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to extract <tt>tempo</tt> and <tt>song_hotttnesss</tt>, here is an example of how to do that on the subset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tempo = []\n",
    "hotness = []\n",
    "\n",
    "files = glob.glob(\"data/MillionSongSubset/data/A\" + \"/[A-Z]/[A-Z]/*\")\n",
    "for f in files:\n",
    "    tempo.append(pd.read_hdf(f,\"/analysis/songs\")[\"tempo\"][0])\n",
    "    hotness.append(pd.read_hdf(f,\"/metadata/songs\")[\"song_hotttnesss\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tempo = np.asarray(tempo)\n",
    "hotness = np.asarray(hotness)\n",
    "print(tempo)\n",
    "print(hotness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of tracks =\", len(files))\n",
    "print(\"with missing hotness values =\", np.sum(np.isnan(hotness)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already got 3268 unknown hotness values and we only tested on a subset of 7620 song, so we can expect to have that information for only a little over half of our final dataset. Maybe we won't use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the files are accessible on the cluster, we will have to go through our SHS dataset and get those attributes for each track_id.\n",
    "We will do so in the following way : (the paths are just examples on the subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tempo = []\n",
    "hotness = []\n",
    "\n",
    "my_file = Path(\"/path/to/file\")\n",
    "for track in covers[\"trackID\"]:\n",
    "    folder1 = track[2]\n",
    "    folder2 = track[3]\n",
    "    folder3 = track[4]\n",
    "    folder_path = \"data/MillionSongSubset/data/\" + folder1 + \"/\" + folder2 + \"/\" + folder3 + \"/\"\n",
    "    track_path = folder_path + track + \".h5\"\n",
    "    if Path(track_path).exists(): #to delete later\n",
    "        tempo.append(pd.read_hdf(track_path,\"/analysis/songs\")[\"tempo\"][0])\n",
    "        hotness.append(pd.read_hdf(track_path,\"/metadata/songs\")[\"song_hotttnesss\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tempo))\n",
    "print(np.sum(~np.isnan(hotness)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, we only found 204 of those tracks in the subset and 128 of them have a hotness value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Determine artist location for spatial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load Additional files\n",
    "#unique_artists=pd.read_csv('data/AdditionalFiles/unique_artists.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['artistID','artistMID','randomTrack','name'])\n",
    "unique_artists=pd.read_csv('data/AdditionalFiles/unique_artists.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['artistID','artistMID','randomTrack','name'])\n",
    "artist_location=pd.read_csv('data/AdditionalFiles/artist_location.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['artistID','lat','long','name','location'])\n",
    "artist_location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load a subset of Second Hand Song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_shs_files(pathToFile):\n",
    "    f = open(pathToFile)\n",
    "    s = StringIO()\n",
    "    cur_ID = None\n",
    "    for ln in f:\n",
    "        if not ln.strip():\n",
    "                continue\n",
    "        if ln.startswith('%'):\n",
    "                cur_ID = ln.replace('\\n','<SEP>',1)\n",
    "                continue\n",
    "        if cur_ID is None:\n",
    "                print ('NO ID found')\n",
    "                sys.exit(1)\n",
    "        s.write(cur_ID + ln)\n",
    "    s.seek(0)\n",
    "    df = pd.read_csv(s,delimiter='<SEP>',engine='python',header=None,names=['shsID','trackID','artistID','shsPerf'])\n",
    "    return df[['trackID', 'artistID', 'shsPerf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the artists' names using the unique_artists.txt file and we assign a location for each track using the artist_location.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(x) : \n",
    "    if x in artist_location.index:\n",
    "        return artist_location.get_value(x, 'location')\n",
    "    else : \n",
    "        return np.nan\n",
    "    \n",
    "data=read_shs_files('data/SHS_testset.txt')\n",
    "data['artist'] = data['artistID'].map(lambda x : unique_artists.get_value(x, 'name'))\n",
    "data['location'] = data['artistID'].map(lambda x : get_location(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the function finding the country for each location. In order to do that we wille use three different python packages : pycountry, us, and geopy, as geopy.geocoders does not support too much requests. \n",
    "\n",
    "- First, we will use the pycountry package to extract countries if location contains one. \n",
    "\n",
    "\n",
    "- If we didn't match any country in pycountry, we will use the us package to check if a us state is present in the location. From the data, we have observed that if the location refer to a us state, the location is either only defined by the state, or the state is the last element of the location.\n",
    "\n",
    "\n",
    "- If the two precedent methods does not succeed, we will use the geopy.geocoders package, using Nominatim( ).\n",
    "\n",
    "\n",
    "- We will manually define countries for some location as they are sometimes mispelled, troncated or refer to a website link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geolocator = Nominatim()\n",
    "\n",
    "def get_country(x):\n",
    "    if x == np.nan:\n",
    "        return x\n",
    "    x = x.replace(\"-\", \",\")\n",
    "    for c in pycountry.countries:\n",
    "        if \"England\" in x or \"UK\" in x: \n",
    "            return \"United Kingdom\"\n",
    "        elif c.name.lower() in x.lower():\n",
    "            return c.name\n",
    "    refactorlast = x.split(\",\")[-1].replace(\" \", \"\")\n",
    "    refactorfirst = x.split(\",\")[0]\n",
    "    usstatelast = us.states.lookup(refactorlast)\n",
    "    usstatefirst = us.states.lookup(refactorfirst)\n",
    "    if usstatelast != None or usstatefirst != None:\n",
    "        return \"United State of America\"\n",
    "    elif x == \"Swingtown\":\n",
    "        return \"United State of America\"\n",
    "    elif x == \"<a href=\\\"http://billyidol.net\\\" onmousedown='UntrustedLink.bootstrap($(this), \\\"fc44f8f60d13ab68c56b3c6709c6d670\\\", event)' target=\\\"_blank\\\" rel=\\\"nofollow\\\">http://billyidol.net</a>\":\n",
    "        return \"United Kingdom\"\n",
    "    elif x == \"Lennox Castle, Glasgow\" or x == \"Knowle West, Bristol, Avon, Engla\"\\\n",
    "        or x == \"Goldsmith's College, Lewisham, Lo\" or x == \"Julian Lennon&#039;s Official Facebook Music Page\"\\\n",
    "        or x == \"Sydney, Moscow, Pressburg\" or x == \"Penarth, Wales to Los Angeles\" or x == \"Leicester, Leicestershire, Englan\":\n",
    "        return \"United Kingdom\"\n",
    "    elif x == \"Vancouver, British Columbia, Cana\":\n",
    "        return \"Canada\"\n",
    "    elif x == \"Washington DC\" or x == \"Philladelphia\" or \"New Jersey\" in x:\n",
    "        return \"United State of America\"\n",
    "    elif \"Czechoslovakia\" in x :\n",
    "        return \"ƒåesko\"\n",
    "    elif x == \"Jaded Heart Town\":\n",
    "        return \"Germany\"\n",
    "    elif x == \"RU\" or x == \"Russia\":\n",
    "        return \"Russia\"\n",
    "    else :\n",
    "        location = geolocator.geocode(x, timeout=None)\n",
    "        return location.address.split(\",\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data['country'] = data['location'].map(lambda x : get_country(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only problem with geopy is that it returns a country in its native language. To uniform our data, we create a function that translates manually the countries in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename(x):\n",
    "    if \"Belgi√´ - Belgique - Belgien\" in x:\n",
    "        return \"Belgium\"\n",
    "    elif \"Brasil\" in x:\n",
    "        return \"Brazil\"\n",
    "    elif \"United State\" in x:\n",
    "        return \"United States of America\"\n",
    "    elif \"Italia\" in x:\n",
    "        return \"Italy\"\n",
    "    elif \"Norge\" in x:\n",
    "        return \"Norway\"\n",
    "    elif \"Espa√±a\" in x:\n",
    "        return \"Spain\"\n",
    "    elif \"Nederland\" in x :\n",
    "        return \"Netherlands\"\n",
    "    elif \"Suomi\" in x :\n",
    "        return \"Finland\"\n",
    "    elif \"Sverige\" in x :\n",
    "        return \"Sweden\"\n",
    "    elif \"UK\" in x :\n",
    "        return \"United Kingdom\"\n",
    "    elif x[0] == \" \":\n",
    "        return x[1:]\n",
    "    else : \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data['country'] = data['country'].map(lambda x : rename(x))\n",
    "#pickle.dump(data, open( \"data.p\", \"wb\" ) )\n",
    "data_country = pickle.load(open(\"data/data_country.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Addition of the genre for each track (Use of LastFM dataset and external website for genre listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the genre of a song, we will use the LastFM dataset that contains a list a tags for each song.\n",
    "Since the dataset is from the MillionSongDataset, we will not use all of the available tracks from LastFM but, but only the ones contained in the SecondHandSong dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the files if they are in the SecondHandSong dataset and create the dataframe\n",
    "covers_df = pickle.load(open(\"data/covers.p\",\"rb\"))\n",
    "list_tracks = covers_df.trackID\n",
    "test_path = \"../../lastfm_test\"\n",
    "train_path = \"../../lastfm_train\"\n",
    "\n",
    "genre_df = pd.DataFrame()\n",
    "def create_dataFrame(genre_df):\n",
    "    for track in list_tracks:\n",
    "        folder1 = track[2]\n",
    "        folder2 = track[3]\n",
    "        folder3 = track[4]\n",
    "        folder_path = \"/\" + folder1 + \"/\" + folder2 + \"/\" + folder3 + \"/\"\n",
    "        track_path = folder_path + track + \".json\"\n",
    "        if glob.glob(train_path + track_path) != []:\n",
    "                genre_df = genre_df.append(pd.DataFrame.from_dict(json.load(open(train_path + track_path)), orient=\"index\").transpose())\n",
    "        elif glob.glob(test_path + folder_path + track) != []:\n",
    "                genre_df = genre_df.append(pd.DataFrame.from_dict(json.load(open(test_path + track_path)), orient=\"index\").transpose())\n",
    "    genre_df = genre_df.reset_index()\n",
    "    return genre_df\n",
    "\n",
    "#tracks_with_tags = create_dataFrame(genre_df)\n",
    "tracks_with_tags = pickle.load(open(\"tracks_with_tags\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now list the unique tags in the resulting dataframe. Due to a time limit for the computation of the matching, we will first test on a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = list()\n",
    "for i in range (0,1000):\n",
    "    tags = tags + tracks_with_tags.tags[i]\n",
    "    \n",
    "tags = np.unique(tags).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of tags contains useless information, thus we first proceed to a pre-cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_tags = {}\n",
    "def clean_tag(x):\n",
    "    clean = x.replace(\"ooo\", \"\")\n",
    "    clean = clean.replace(\"-o\", \"\")\n",
    "    clean = clean.replace(\"o-\", \"\")\n",
    "    clean = clean.replace(\"- \", \"\")\n",
    "    clean = clean.replace(\"-\", \"\")\n",
    "    clean_tags[x] = clean\n",
    "for t in tags:\n",
    "    clean_tag(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order assign a genre to each song, we will use their different tags and try to match it with a list of genre obtained by webscrapping the http://www.musicgenreslist.com website. For more details on the webscrapping see the notebook Genre Webscrapping.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_genres = pickle.load(open(\"data/map_genres\", \"rb\"))\n",
    "all_genres = pickle.load(open(\"data/all_genres.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the Sequence Matcher package to match tags to the web-scrapped genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.80\n",
    "def match_genres():\n",
    "    i = 0\n",
    "    genre_map = {}\n",
    "    no_match = list()\n",
    "    for ind in range(0,len(tags)):\n",
    "        name1 = tags[ind]\n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "        if clean_tags[name1] == \"\":\n",
    "            genre_map[name1] = np.nan\n",
    "        best_ratio = 0\n",
    "        match = \"\"\n",
    "        for name2 in map_genres.keys():\n",
    "            if name2.lower() in name1.lower():\n",
    "                for subgenre in map_genres[name2]:\n",
    "                    ratio = SequenceMatcher(None,name1.lower(),name2.lower()).ratio()\n",
    "                    if ratio > best_ratio:       # we find the maximum similarity\n",
    "                        best_ratio = ratio\n",
    "                        match = name2\n",
    "                if (best_ratio > threshold):     # if it's superior to our threshold we add that couple to the mapping\n",
    "                    genre_map[name1] = match\n",
    "                else:\n",
    "                    genre_map[name1] = name2\n",
    "        if match == \"\":\n",
    "            for subgenre in all_genres:\n",
    "                ratio = SequenceMatcher(None,name1.lower(),name2.lower()).ratio()\n",
    "                if ratio > best_ratio:       # we find the maximum similarity\n",
    "                    best_ratio = ratio\n",
    "                    match = name2\n",
    "            if (best_ratio > threshold):     # if it's superior to our threshold we add that couple to the mapping\n",
    "                genre_map[name1] = match\n",
    "            else :\n",
    "                genre_map[name1] = np.nan\n",
    "        i = i+1\n",
    "    return (genre_map, no_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
