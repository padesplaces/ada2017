{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT - *My Way* of seeing music covers\n",
    "#### Pierre-Antoine Desplaces, Anaïs Ladoy, Lou Richard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from io import StringIO\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook plan\n",
    "1. Data importation\n",
    "2. Clique organisation (Multi-level indexing)\n",
    "3. Addition of the language and the year of each track (SHS website web-scraping)\n",
    "4. Data Wrangling and Find original song\n",
    "5. Addition of the genres and artist location\n",
    "6. Addition of the tempo and song hotness of each track (Access to track files through the cluster)\n",
    "7. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data importation\n",
    "Download available additional files containing metadata about our dataset from the cluster (dataset/million-songs_untar/)\n",
    "- tracks_per_year.txt\n",
    "- unique_tracks.txt\n",
    "- unique_artists.txt\n",
    "- artist_location.txt\n",
    "\n",
    "Use the Second Hand Songs (SHS) dataset that was created through a collaboration between the Million Songs team and the Second Hand Songs website (https://secondhandsongs.com/). These data are splitted into two datasets to allowed machine learnings algorithms (a train and a test set).\n",
    "- SHS_testset.txt\n",
    "- SHS_trainset.txt\n",
    "Since we don't need this distinction for our data analysis, we merged these two datasets.\n",
    "\n",
    "The use of external dataset (LastFM) for the genres and the use of the track files (.h5) available through the cluster are commented in part 4 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some general informations about our data :\n",
    "- All the additional files were downloaded from the cluster giving all the metadata of the Million Songs dataset. They will help to elaborate a plan and a script will then search more information about a specific track (h5 files in the cluster) maybe using cluster cpu. The path to access to a track in the cluster is for example million-songs/data/A/A/A (with the 3 letters at the end being the 3rd, 4th and 5th letter on the track id).\n",
    "- The music covers will be detected using another dataset (SecondHandSongs), we have the choice to use the downloadable dataset containing 18,196 tracks (all with a connection to the MSD dataset), or to web-scrapp the SHS website (https://secondhandsongs.com/) where we have much more information (522 436 covers) but not necessarly connected to our MSD dataset. The SHS API is RESTful (return a JSON object) and will be used to provide additional or missing informations (localisation, language of the song, ...) in our dataset.\n",
    "- Some artist are geolocalised (30% of the MSD total artists) on the artist_location dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Additional files\n",
    "tracks_per_year=pd.read_csv('data/AdditionalFiles/tracks_per_year.txt',delimiter='<SEP>',engine='python',header=None,index_col=1,names=['year','trackID','artist','title'])\n",
    "unique_tracks=pd.read_csv('data/AdditionalFiles/unique_tracks.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['trackID','songID','artist','title'])\n",
    "unique_artists=pd.read_csv('data/AdditionalFiles/unique_artists.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['artistID','artistMID','randomTrack','name'])\n",
    "artist_location=pd.read_csv('data/AdditionalFiles/artist_location.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['artistID','lat','long','name','location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe (Unique index, Number of elements)\n",
      "tracks_per_year  (True, 515576)\n",
      "unique_tracks  (True, 1000000)\n",
      "unique_artists  (True, 44745)\n",
      "artist_location  (True, 13850)\n"
     ]
    }
   ],
   "source": [
    "#Check if indexes is unique and print the number of elements for each dataframe\n",
    "print('Dataframe (Unique index, Number of elements)')\n",
    "print('tracks_per_year ',(tracks_per_year.index.is_unique,tracks_per_year.shape[0]))\n",
    "print('unique_tracks ',(unique_tracks.index.is_unique,unique_tracks.shape[0]))\n",
    "print('unique_artists ',(unique_artists.index.is_unique,unique_artists.shape[0]))\n",
    "print('artist_location ',(artist_location.index.is_unique,artist_location.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covers dataset (SHS_testset.txt and SHS_trainset.txt) were organised in a very special way where group (named \"cliques\") list some tracks that are interrelated (music covers and original track). The function **read_shs_files** is used to import the files keeping the \"clique\" configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_shs_files(pathToFile):\n",
    "    f = open(pathToFile)\n",
    "    s = StringIO()\n",
    "    cur_ID = None\n",
    "    for ln in f:\n",
    "        if not ln.strip():\n",
    "                continue\n",
    "        if ln.startswith('%'):\n",
    "                cur_ID = ln.replace('\\n','<SEP>',1)\n",
    "                continue\n",
    "        if cur_ID is None:\n",
    "                print ('NO ID found')\n",
    "                sys.exit(1)\n",
    "        s.write(cur_ID + ln)\n",
    "    s.seek(0)\n",
    "    df = pd.read_csv(s,delimiter='<SEP>',engine='python',header=None,names=['shsID','trackID','artistID','shsPerf'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shsID</th>\n",
       "      <th>trackID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>shsPerf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115402,74782, Putty (In Your Hands)</td>\n",
       "      <td>TRJVDMI128F4281B99</td>\n",
       "      <td>AR46LG01187B98DB5D</td>\n",
       "      <td>74784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115402,74782, Putty (In Your Hands)</td>\n",
       "      <td>TRNJXCO128F92E1930</td>\n",
       "      <td>ARQD13K1187B98E441</td>\n",
       "      <td>138584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24350, I.G.Y. (Album Version)</td>\n",
       "      <td>TRIBOIS128F9340B19</td>\n",
       "      <td>ARUVZYG1187B9B2809</td>\n",
       "      <td>24350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24350, I.G.Y. (Album Version)</td>\n",
       "      <td>TRGXZDU128F9301E53</td>\n",
       "      <td>AR4LE591187FB3FCFB</td>\n",
       "      <td>24363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79178, When The Catfish Is In Bloom</td>\n",
       "      <td>TRQSIOY128F92FACA7</td>\n",
       "      <td>ARU75JD1187FB38B79</td>\n",
       "      <td>79178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 shsID             trackID  \\\n",
       "0  115402,74782, Putty (In Your Hands)  TRJVDMI128F4281B99   \n",
       "1  115402,74782, Putty (In Your Hands)  TRNJXCO128F92E1930   \n",
       "2        24350, I.G.Y. (Album Version)  TRIBOIS128F9340B19   \n",
       "3        24350, I.G.Y. (Album Version)  TRGXZDU128F9301E53   \n",
       "4  79178, When The Catfish Is In Bloom  TRQSIOY128F92FACA7   \n",
       "\n",
       "             artistID  shsPerf  \n",
       "0  AR46LG01187B98DB5D    74784  \n",
       "1  ARQD13K1187B98E441   138584  \n",
       "2  ARUVZYG1187B9B2809    24350  \n",
       "3  AR4LE591187FB3FCFB    24363  \n",
       "4  ARU75JD1187FB38B79    79178  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the two SHS datasets and concatenate them\n",
    "SHS_testset=read_shs_files('data/SHS_testset.txt')\n",
    "SHS_trainset=read_shs_files('data/SHS_trainset.txt')\n",
    "covers=pd.concat([SHS_testset,SHS_trainset])\n",
    "covers.shsID=covers.shsID.str.strip('%')\n",
    "covers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said before, our dataset was created from a collaboration between the Million Songs Dataset (MSD) and the Second Hand Songs Dataset (SHS).  \n",
    "\n",
    "The MSD consists of almost all the information available through the Echo Nest API for one million popular tracks and the *trackID* and *artistID* are directly based on the Echo Nest structure.   \n",
    "More specifically, the *trackID* is the unique identifier of a track (connection with unique_tracks.txt) and the *artistID* is the unique identifier of an artist (connection with unique_artists.txt). The *trackID* is also the path used to navigate through the MSD directory and access to a specific song (through the cluster).  \n",
    "\n",
    "The *shsPerf* corresponds to the SHS identifier of a song and this information is sometimes missing. Exploring the SHS website, we found that the page of a specific song can be accessed with different paths using the *shsPerf* information :\n",
    "- https://secondhandsongs.com/performance/ [shsPerf]\n",
    "- https://secondhandsongs.com/work/ [shsPerf]\n",
    "\n",
    "We have deducted that only original music song has both a work page and a performance page, covers having only performance page. In some cases, the *shsPerf* for an original music song is not the same if we want to reach the performance page or the work page and in our dataset, we assumed that the *shsPerf* was only performance id.\n",
    "\n",
    "The structure of the first column was impossible to decode and the informations contained as the name of the song can be found elsewhere (unique_tracks.txt). Thus, the column will only be used to define a clique_id for each track. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert shsID to clique id (first convert to category and get a code)\n",
    "covers=covers.assign(clique_id=(covers.shsID.astype('category')).cat.codes)\n",
    "#Remove the shsID column (useless since we have the clique_id now)\n",
    "covers.drop('shsID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the informations contained in the metadata files first, we merged some necessary attributes (name of the artist, title of the track, released date) from the MSD dataframes named before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge with unique_artists dataframe to find the artist name for each track (no taking consideration of featuring since we take only the name of the artist assigned with the track)\n",
    "covers=covers.merge(unique_artists[['name']],how='left',left_on='artistID',right_index=True)\n",
    "#Merge with unique_tracks dataframe to find the track name\n",
    "covers=covers.merge(unique_tracks[['title']],how='left',left_on='trackID',right_index=True)\n",
    "#Merge with tracks_per_year dataframe to find the year of each track\n",
    "covers=covers.merge(tracks_per_year[['year']],how='left',left_on='trackID',right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covers=covers.sort_values(['clique_id', 'year'], ascending=[True, True]).reset_index() #Reset index according clique_id and year\n",
    "covers.drop('index',axis=1,inplace=True) #Drop the previous index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trackID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>shsPerf</th>\n",
       "      <th>clique_id</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRGDMZP128F42BC52B</td>\n",
       "      <td>ARB1DDF1187FB4FCFB</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>Louis Armstrong</td>\n",
       "      <td>Stardust</td>\n",
       "      <td>1988.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRCATYW12903D038FE</td>\n",
       "      <td>ARGJEEO1271F573FD6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>Artie Shaw and his orchestra</td>\n",
       "      <td>Stardust</td>\n",
       "      <td>1988.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRVMZJZ128F4270CE4</td>\n",
       "      <td>ARY0HTV1187FB4A1B1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hoagy Carmichael</td>\n",
       "      <td>Star Dust</td>\n",
       "      <td>1999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRKOINL128F42926C3</td>\n",
       "      <td>ARQ5FSZ1187B98AD74</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>Connee Boswell &amp; Sy Oliver Orchestra</td>\n",
       "      <td>Star Dust</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TROJZTF128F428B546</td>\n",
       "      <td>ARJN76O1187FB43C99</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ana Belén</td>\n",
       "      <td>Yo Vengo A Ofrecer Mi Corazon</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              trackID            artistID  shsPerf  clique_id  \\\n",
       "0  TRGDMZP128F42BC52B  ARB1DDF1187FB4FCFB       -1          0   \n",
       "1  TRCATYW12903D038FE  ARGJEEO1271F573FD6       -1          0   \n",
       "2  TRVMZJZ128F4270CE4  ARY0HTV1187FB4A1B1       -1          0   \n",
       "3  TRKOINL128F42926C3  ARQ5FSZ1187B98AD74       -1          0   \n",
       "4  TROJZTF128F428B546  ARJN76O1187FB43C99       -1          1   \n",
       "\n",
       "                                   name                          title    year  \n",
       "0                       Louis Armstrong                       Stardust  1988.0  \n",
       "1          Artie Shaw and his orchestra                       Stardust  1988.0  \n",
       "2                      Hoagy Carmichael                      Star Dust  1999.0  \n",
       "3  Connee Boswell & Sy Oliver Orchestra                      Star Dust     NaN  \n",
       "4                             Ana Belén  Yo Vengo A Ofrecer Mi Corazon  2001.0  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is printed some useful informations about the cover dataset (the basis of our work) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tracks : 18196\n",
      "Number of cliques : 5854\n",
      "Number of unique tracks : 18196\n",
      "Number of unique artists : 5578\n",
      "Number of missing trackID : 0\n",
      "Number of missing artistID : 0\n",
      "Number of missing years : 4796\n",
      "Number of invalid shsPerf : 3075\n"
     ]
    }
   ],
   "source": [
    "print('Number of tracks :', covers.shape[0])\n",
    "print('Number of cliques :', len(covers.clique_id.unique()))\n",
    "print('Number of unique tracks :', len(covers.trackID.unique())) \n",
    "print('Number of unique artists :', len(covers.artistID.unique()))\n",
    "print('Number of missing trackID :', len(covers[covers.trackID.isnull()]))\n",
    "print('Number of missing artistID :', len(covers[covers.artistID.isnull()]))\n",
    "print('Number of missing years :', len(covers[covers.year.isnull()]))\n",
    "print('Number of invalid shsPerf :', len(covers[covers.shsPerf<0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important step that we will faced in the data wrangling process will be to differentiate the original song and the music covers inside each clique.  \n",
    "We thought using the released year that is provided in the track_per_year.txt file but with 4796 tracks (26%) with missing years, we need to find another way to get them. Furthermore, year isn't necessarly sufficient informations to discriminate the tracks (cover appears sometimes in the same year than the original one), thus it will be better to have the released date for ALL the tracks if the information is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Addition of the language and the year of each track (SHS website web-scraping)\n",
    "\n",
    "Some useful informations can be found in a music cover page\n",
    "In the Second Hand Song website (https://secondhandsongs.com/), each song page can be access by a specific id (shsPerf in our dataset) and some useful information about the song is provided as its language and its released date (not only the year). Furthermore, in case of music cover page, a link to the original song page is also present.\n",
    "\n",
    "Thus, we decided to improve our dataset using web-scrapping of the SHS website.\n",
    "\n",
    "As said before, the performance id (that is used in the URL to access to the song page) is available in our cover dataframe (shsPerf) but 3075 tracks (17%) have unvalid shsPerf (shsPerf=-1) and that make access to their page impossible.  \n",
    "Thus, we have two ways to access extract the language/year/original song via web-scrapping :\n",
    "- For valid SHS performance ID, access to the performance page (e.g. 'https://secondhandsongs.com/performance/1983') and web-scrapping of the Language and Released date informations using the perfInfo() function.\n",
    "- For invalid SHS performance ID, API request to the search page (e.g. 'https://secondhandsongs.com/search/performance?title=blackbird&performer=beatles'), extract the perf ID with the find_PerfID() and then use the perfInfo() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18196, 7)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the artist is different if we use the information provided in the unique_artists.txt or in the unique_tracks.txt. Indeed, in order to assign a specific *artistID* to each track, the MSD team has splitted the featuring tracks keeping in an arbitrary way one of the artist.  \n",
    "Both informations will be useful depending the precision we want to use in our algorithms, that's why two field corresponds to artist names (*name* and *artist*) in our cover dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge with the unique_tracks dataframe to get the name of the artist for the track (take featuring as well)\n",
    "covers=covers.merge(unique_tracks[['artist']],how='left',left_on='trackID',right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# API request to find the shsPerf for unvalid ones (shsPerf=-1)\n",
    "# Corresponds to a detailed search in the SHS website with a contains condition on title and on artist\n",
    "\n",
    "def find_shsPerf(x):\n",
    "    \n",
    "    # x corresponds to the index position of each track (in order to execute the function on the entire dataframe)\n",
    "    title=covers.iloc[x]['title'] \n",
    "    artist=covers.iloc[x]['name']\n",
    "    shsPerf=covers.iloc[x]['shsPerf']\n",
    "    \n",
    "    # In case of unvalid (missing) shsPerf\n",
    "    if shsPerf<0: \n",
    "        title=title.replace('.', '').replace('_', '').replace('/', '').lower().replace(' ','+')\n",
    "        artist=artist.replace('.', '').replace('_', '').replace('/', '').lower().replace(' ','+')\n",
    "        r=requests.get('https://secondhandsongs.com/search/performance?title='+title+'&op_title=contains&performer='+artist+'&op_performer=contains')\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        results=soup.find('tbody')\n",
    "\n",
    "        if results is None :\n",
    "            # Assign default value of 0 if no results found in the detailed search request\n",
    "            new_shsPerf=0\n",
    "        else:\n",
    "            # Take the first result of the detailed search (since it is sorted according relevance)\n",
    "            new_shsPerf=int(results.find('a',attrs={'class':'link-performance'})['href'].split('/')[2])\n",
    "    \n",
    "    # Keep as it is if the shsPerf is valid\n",
    "    else :\n",
    "        new_shsPerf=shsPerf\n",
    "        \n",
    "        \n",
    "    return new_shsPerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the shsPerf for the tracks which doesn't have valid ones\n",
    "#covers_withSHS=covers_p.shsPerf.index.map(lambda x: find_shsPerf(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(covers_withSHS,open('data/covers_1.p','wb'))\n",
    "covers['shsPerf']=pickle.load(open(\"data/covers_1.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of missing shsPerf in our dataset\n",
    "len(covers[covers.shsPerf==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still can't access the SHS pages for **1088** music covers (6%) and the missing *shsPerf* are assigned to a default value of 0. We will need to decide if remove them since it will be impossible to find the missing release date and/or the language of the track.\n",
    "For now, we will compute the perfInfo_SHS for all the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# API request to extract the language, the released date and the shsPerf of the original song\n",
    "# Computes for all the tracks in our dataset\n",
    "\n",
    "def perfInfo_SHS(shsPerf):\n",
    "        \n",
    "    # If the shsPerf is missing in our dataset assign default values \"Unavailable\" for the three fields\n",
    "    if shsPerf==0:\n",
    "        perfLanguage='Unavailable'\n",
    "        perfDate='Unavailable'\n",
    "        original_shsPerf='Unavailable'\n",
    "     \n",
    "    # If we have shsPerf information in our dataset\n",
    "    else :\n",
    "        r = requests.get('https://secondhandsongs.com/performance/'+str(shsPerf)) # Access to the song page on SHS\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        perfMeta=soup.find('dl',attrs={'class':'dl-horizontal'})\n",
    "        \n",
    "        # If no metadata of the song is found, assign default values \"Missing\" to differentiate unreachable informations\n",
    "        # and missing ones.\n",
    "        if perfMeta is None:\n",
    "            perfLanguage='Missing'\n",
    "            perfDate='Missing'\n",
    "            original_shsPerf='Missing'\n",
    "        else :\n",
    "            # Extract language\n",
    "            perfLanguage=perfMeta.find('dd',attrs={'itemprop':'inLanguage'})\n",
    "            if perfLanguage is None :\n",
    "                perfLanguage='Missing'\n",
    "            else :\n",
    "                perfLanguage=perfLanguage.text\n",
    "\n",
    "            # Extract released date    \n",
    "            perfDate=perfMeta.find('div',attrs={'class':'media-body'})\n",
    "            if perfDate is None :\n",
    "                perfDate='Missing'\n",
    "            else :\n",
    "                perfDate=perfDate.find('p').text.split('\\n')[2].strip(' ')\n",
    "\n",
    "            #Extract original shsPerf (work or performance ID) \n",
    "            original_section=soup.find('section',attrs={'class':'work-originals'})\n",
    "            versions_section=soup.find('section',attrs={'id':'entity-section'})\n",
    "            \n",
    "            if original_section is None :\n",
    "                if versions_section is None :\n",
    "                    original_shsPerf='Missing'\n",
    "                else :\n",
    "                    \n",
    "                    original_shsPerf=versions_section.find('a')['href']\n",
    "                    if original_shsPerf is None :\n",
    "                        original_shsPerf='Missing'\n",
    "                    else :\n",
    "                        original_shsPerf=original_shsPerf.split('/')[2]\n",
    "\n",
    "            else :\n",
    "                original_shsPerf=original_section.find('div',attrs={'class':'media-body'})\n",
    "                original_shsWork=original_section.find('a',attrs={'class':'link-work'})['href']\n",
    "                \n",
    "                if original_shsPerf is None :\n",
    "                    original_shsPerf=original_shsWork.split('/')[2]\n",
    "                else :\n",
    "                    original_shsPerf=original_shsPerf.find('a')['href'].split('/')[2]\n",
    "\n",
    "    return perfLanguage,perfDate,original_shsPerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add informations found with web-scrapping\n",
    "#covers['language'], \\\n",
    "#covers['date'], \\\n",
    "#covers['original_shsPerf']= zip(*covers.shsPerf.map(perfInfo_SHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trackID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>shsPerf</th>\n",
       "      <th>clique_id</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRGDMZP128F42BC52B</td>\n",
       "      <td>ARB1DDF1187FB4FCFB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Louis Armstrong</td>\n",
       "      <td>Stardust</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>Louis Armstrong &amp; His Orchestra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRCATYW12903D038FE</td>\n",
       "      <td>ARGJEEO1271F573FD6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Artie Shaw and his orchestra</td>\n",
       "      <td>Stardust</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>Artie Shaw and his orchestra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRVMZJZ128F4270CE4</td>\n",
       "      <td>ARY0HTV1187FB4A1B1</td>\n",
       "      <td>412972</td>\n",
       "      <td>0</td>\n",
       "      <td>Hoagy Carmichael</td>\n",
       "      <td>Star Dust</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Hoagy Carmichael</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRKOINL128F42926C3</td>\n",
       "      <td>ARQ5FSZ1187B98AD74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Connee Boswell &amp; Sy Oliver Orchestra</td>\n",
       "      <td>Star Dust</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Connee Boswell &amp; Sy Oliver Orchestra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TROJZTF128F428B546</td>\n",
       "      <td>ARJN76O1187FB43C99</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ana Belén</td>\n",
       "      <td>Yo Vengo A Ofrecer Mi Corazon</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Ana Belén</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              trackID            artistID  shsPerf  clique_id  \\\n",
       "0  TRGDMZP128F42BC52B  ARB1DDF1187FB4FCFB        0          0   \n",
       "1  TRCATYW12903D038FE  ARGJEEO1271F573FD6        0          0   \n",
       "2  TRVMZJZ128F4270CE4  ARY0HTV1187FB4A1B1   412972          0   \n",
       "3  TRKOINL128F42926C3  ARQ5FSZ1187B98AD74        0          0   \n",
       "4  TROJZTF128F428B546  ARJN76O1187FB43C99        0          1   \n",
       "\n",
       "                                   name                          title  \\\n",
       "0                       Louis Armstrong                       Stardust   \n",
       "1          Artie Shaw and his orchestra                       Stardust   \n",
       "2                      Hoagy Carmichael                      Star Dust   \n",
       "3  Connee Boswell & Sy Oliver Orchestra                      Star Dust   \n",
       "4                             Ana Belén  Yo Vengo A Ofrecer Mi Corazon   \n",
       "\n",
       "     year                                artist  \n",
       "0  1988.0       Louis Armstrong & His Orchestra  \n",
       "1  1988.0          Artie Shaw and his orchestra  \n",
       "2  1999.0                      Hoagy Carmichael  \n",
       "3     NaN  Connee Boswell & Sy Oliver Orchestra  \n",
       "4  2001.0                             Ana Belén  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(covers,open('data/covers_2.p','wb'))\n",
    "covers=pickle.load(open(\"data/covers_2.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unavailable language :  1088\n",
      "Number of missing language :  1426\n",
      "Total number of tracks with no language information :  2514\n",
      "\n",
      "Number of unavailable released date : 1088\n",
      "Number of missing released date : 159\n",
      "Total number of tracks with no released date :  1247\n",
      "\n",
      "Number of unavailable original shsPerf : 1088\n",
      "Number of missing original shsPerf : 145\n",
      "Total number of tracks with no original shsPerf information :  1233\n"
     ]
    }
   ],
   "source": [
    "print('Number of unavailable language : ', len(covers[covers.language=='Unavailable']) )\n",
    "print('Number of missing language : ', len(covers[covers.language=='Missing']) )\n",
    "print('Total number of tracks with no language information : ', len((covers[covers.language=='Missing']) | (covers[covers.language=='Unavailable'])))\n",
    "print('')\n",
    "print('Number of unavailable released date :' ,len(covers[covers.date=='Unavailable']))\n",
    "print('Number of missing released date :' ,len(covers[covers.date=='Missing']))\n",
    "print('Total number of tracks with no released date : ', len((covers[covers.date=='Missing']) | (covers[covers.original_shsPerf=='Unavailable']))) \n",
    "print('')\n",
    "print('Number of unavailable original shsPerf :' ,len(covers[covers.original_shsPerf=='Unavailable']))\n",
    "print('Number of missing original shsPerf :' ,len(covers[covers.original_shsPerf=='Missing']))\n",
    "print('Total number of tracks with no original shsPerf information : ', len((covers[covers.original_shsPerf=='Missing']) | (covers[covers.original_shsPerf=='Unavailable']))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find the original song\n",
    "\n",
    "After the extraction of the different elements, we noticed several problems :\n",
    "\n",
    "1\\. The track year (year) is sometimes different form the released date (date) we've extracted from the SHS website, we will prefer the data information found in the SHS website after some manual verifications.\n",
    "\n",
    "\n",
    "2\\. We found several cases where language (14%) and/or released year (7%) information were missing or unavailable. Since we have a small dataset, we cannot allowed to delete a lot of tracks so we kept these tracks and handle this issue in the data analysis.\n",
    "\n",
    "\n",
    "3\\. A total of 1233 tracks (7%) don't have original shsPerf information so we'll need to face this issue in our algorithm to find the original track for each clique.\n",
    "\n",
    "\n",
    "4\\. We also noticed that in case of cliques with 2 elements and with one having shsPerf=0, if the other one has an original shsPerf, we couldn't link to the other song present in the clique (and it's generally the original one). Thus, we created a special algorithm **attribute_original()** to adress this issue.\n",
    "\n",
    "\n",
    "5\\. Some original performance that were found in the SHS website don't appear in the clique. And we can extract all informations in the SHS website so we decided to choose the earlier released song as original in this case. Furthermore, in some cases, several original shsPerf are returned inside one clique. We created an algorithm **find_original_track()** that return a unique shsPerf that is considered as the original track for each clique.\n",
    "\n",
    "\n",
    "6\\. There are duplicated shsPerf in our dataset, it means that we've extracted the same informations from the SHS website (*original shsPerf*, *language* and *released year*) for these tracks. We have checked the concerned tracks and it is mostly duplicated tracks (although they have different *trackID* in the Million Song Dataset). Since they will bias our statistics, we decided to remove the duplicated shsPerf, keeping the one with the fewest missing fields.\n",
    "The function **duplicate_to_keep()** handles this issue.\n",
    "\n",
    "\n",
    "7\\. Same original shsPerf were found in different cliques, and one *original shsPerf* was linking to a track in another clique. After some verifications, we noticed that some cliques needed to be merged. We created a function **merge_cliques()** that handles this issue.\n",
    "\n",
    "\n",
    "8\\. In cliques where no original shsPerf is found and where released date for at least one track is present, it is impossible to define an original song. Thus, these cliques need to be removed from our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve ISSUE n°1\n",
    "# Merge the informations concerning the released date (from MSD and SHS) keeping SHS information in priority\n",
    "covers['date'] = covers.apply(lambda row: row['year'] if ((row['date']=='Missing') | (row['date']=='Unavailable')) else row['date'],axis=1)\n",
    "covers.drop('year',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tracks without released date information after the merged : 358\n"
     ]
    }
   ],
   "source": [
    "print('Number of tracks without released date information after the merged :', len(covers[covers.date.isnull()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put all the missing shsPerf (unavailable or missing) to 0 and convert to int\n",
    "covers['original_shsPerf'] = np.where(((covers['original_shsPerf']=='Unavailable') | (covers['original_shsPerf']=='Missing')), 0, covers['original_shsPerf'])\n",
    "covers['original_shsPerf']=covers['original_shsPerf'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>original_shsPerf</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clique_id</th>\n",
       "      <th>freq</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <td>16660</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>1</th>\n",
       "      <td>142889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                original_shsPerf  freq\n",
       "clique_id freq                        \n",
       "0         3                    0     3\n",
       "          1                19677     1\n",
       "1         2                    0     2\n",
       "2         2                16660     2\n",
       "3         1               142889     1"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the frequency for each original shsPerf and sort values according frequency\n",
    "freq_original=covers.groupby(['clique_id','original_shsPerf'],as_index=False)['clique_id'].agg({'freq':'count'})\n",
    "freq_original.sort_values(['clique_id', 'freq'], ascending=[True, False],inplace=True)\n",
    "freq_original.set_index(['clique_id', 'freq'],drop=False,inplace=True)\n",
    "freq_original.drop('clique_id',axis=1,inplace=True)\n",
    "freq_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anaisladoy/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py:2946: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  results[name] = obj.aggregate(func)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nbrow</th>\n",
       "      <th>nbfreq</th>\n",
       "      <th>clique_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           nbrow  nbfreq  clique_id\n",
       "clique_id                          \n",
       "0              2       4          0\n",
       "1              1       2          1\n",
       "2              1       2          2\n",
       "3              2       2          3\n",
       "4              1       2          4"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each clique, count the number of different original shsPerf (including Missing or Unavailable informations)\n",
    "# and the total number of tracks contained in the clique\n",
    "freq_original_agg=freq_original[['freq']].groupby(by='clique_id').agg([{'nbrow':'count','nbfreq':'sum'}])\n",
    "freq_original_agg.columns=freq_original_agg.columns.droplevel()\n",
    "freq_original_agg.columns=freq_original_agg.columns.droplevel()\n",
    "freq_original_agg['clique_id']=freq_original_agg.index\n",
    "freq_original_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nbrow</th>\n",
       "      <th>nbfreq</th>\n",
       "      <th>clique_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           nbrow  nbfreq  clique_id\n",
       "clique_id                          \n",
       "3              2       2          3\n",
       "6              2       2          6\n",
       "8              2       2          8\n",
       "12             2       2         12\n",
       "13             2       2         13"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resolve ISSUE n°4\n",
    "# Keep only clique where there are 2 elements with two different original_shsPerf\n",
    "freq_original_agg=freq_original_agg[(freq_original_agg.nbrow==2) & (freq_original_agg.nbfreq==2)]\n",
    "freq_original_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to resolve ISSUE n°4\n",
    "\n",
    "def attribute_original(clique_id) :\n",
    "\n",
    "    first_elem=covers_clique.loc[clique_id].iloc[0]\n",
    "    second_elem=covers_clique.loc[clique_id].iloc[1]\n",
    "    \n",
    "    if (first_elem.original_shsPerf==0) | (second_elem.original_shsPerf==0) :\n",
    "        if (first_elem.original_shsPerf==0) & (second_elem.original_shsPerf!=second_elem.shsPerf):\n",
    "            elemTrack=first_elem.name\n",
    "            elemSHS=second_elem.original_shsPerf\n",
    "            \n",
    "        elif (second_elem.original_shsPerf==0) & (first_elem.original_shsPerf!=first_elem.shsPerf):\n",
    "            elemTrack=second_elem.name\n",
    "            elemSHS=first_elem.original_shsPerf\n",
    "        else :\n",
    "            elemTrack=np.nan\n",
    "            elemSHS=np.nan\n",
    "            \n",
    "    else :\n",
    "        elemTrack=np.nan\n",
    "        elemSHS=np.nan\n",
    "        \n",
    "    return elemTrack, elemSHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_shs=pd.DataFrame(columns=['elemTrack','elemSHS'])\n",
    "replace_shs['elemTrack'], \\\n",
    "replace_shs['elemSHS'] = zip(*freq_original_agg.clique_id.map(attribute_original))\n",
    "replace_shs.dropna(axis=0,inplace=True)\n",
    "replace_shs.elemSHS=replace_shs.elemSHS.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covers=covers.merge(replace_shs,how='left',left_on='trackID',right_on='elemTrack')\n",
    "covers['shsPerf'] = np.where(covers['elemSHS'].notnull(), covers['elemSHS'], covers['shsPerf'])\n",
    "covers.drop(['elemTrack','elemSHS'],axis=1,inplace=True)\n",
    "covers.shsPerf=covers.shsPerf.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(covers,open('data/covers_3.p','wb'))\n",
    "covers=pickle.load(open(\"data/covers_3.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a multilevel index with clique_id and track_id that will be used to exectute the algorithm to find original song\n",
    "covers_clique=covers.copy()\n",
    "covers_clique.set_index(['clique_id','trackID'],inplace=True)\n",
    "covers_clique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resolve ISSUE n°5\n",
    "\n",
    "# Function to attribute a unique original shsPerf to each clique\n",
    "def find_original_track(clique_id,max_freq_row):\n",
    "    \n",
    "    nb_elems=freq_original.loc[clique_id].original_shsPerf.count()\n",
    "    unique_shsPerf=covers.shsPerf.unique()[1:];\n",
    "\n",
    "    if nb_elems == 1 :\n",
    "\n",
    "        first_elem=max_freq_row.original_shsPerf;\n",
    "        if first_elem !=0:\n",
    "            if (first_elem in(unique_shsPerf))==True :\n",
    "                original_song=first_elem;\n",
    "            else :\n",
    "                original_song='Unknown';\n",
    "        \n",
    "        else :\n",
    "            original_song='Unknown';\n",
    "    \n",
    "    if nb_elems == 2 :\n",
    "        first_elem=max_freq_row.original_shsPerf;\n",
    "        second_elem=freq_original.loc[clique_id].iloc[1].original_shsPerf;\n",
    "        \n",
    "        if first_elem != 0:\n",
    "            if (first_elem in (unique_shsPerf))==True :\n",
    "                original_song=first_elem;\n",
    "            elif (second_elem in (unique_shsPerf))==True :\n",
    "                original_song=second_elem;\n",
    "            else : #Put the reference as the first\n",
    "                original_song='Unknown';\n",
    "        \n",
    "        else :\n",
    "            if (second_elem in (unique_shsPerf))==True :\n",
    "                original_song=second_elem;\n",
    "            else :\n",
    "                original_song='Unknown';\n",
    "    \n",
    "    \n",
    "    elif nb_elems>2 :\n",
    "        first_elem=max_freq_row.original_shsPerf;\n",
    "        \n",
    "        second_elem=freq_original.loc[clique_id].iloc[1].original_shsPerf;\n",
    "        third_elem=freq_original.loc[clique_id].iloc[2].original_shsPerf;\n",
    "        \n",
    "        if first_elem != 0:\n",
    "            if (first_elem in (unique_shsPerf))==True :\n",
    "                original_song=first_elem;\n",
    "            elif (second_elem in (unique_shsPerf))==True :\n",
    "                original_song=second_elem;\n",
    "            elif (third_elem in (unique_shsPerf))==True :\n",
    "                original_song=third_elem;\n",
    "            else :\n",
    "                original_song='Unknown';\n",
    "        \n",
    "        else :\n",
    "            if (second_elem in (unique_shsPerf))==True :\n",
    "                original_song=second_elem;\n",
    "            elif (third_elem in (unique_shsPerf))==True :\n",
    "                original_song=third_elem;\n",
    "            else :\n",
    "                original_song='Unknown';\n",
    "                \n",
    "    \n",
    "    return clique_id, original_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty DataFrame to contain the outputs of find_original_track()\n",
    "original_song_df=pd.DataFrame(columns=['clique_id','original_id'])\n",
    "#Dataframe that find for each clique the shsPerf corresponding to the original tracl\n",
    "original_song_df['clique_id'], \\\n",
    "original_song_df['original_id'] = zip(*pd.Series(freq_original.index.get_level_values('clique_id').unique()).map(lambda x : find_original_track(x,freq_original.loc[x].iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clique where no original songs were found via this first algorithm :  1944\n",
      "Number of different cliques having the same shsPerf for original track (ISSUE n°7):  49\n"
     ]
    }
   ],
   "source": [
    "print('Number of clique where no original songs were found via this first algorithm : ', len(original_song_df[original_song_df.original_id=='Unknown']))\n",
    "print('Number of different cliques having the same shsPerf for original track (ISSUE n°7): ',len(original_song_df[(original_song_df.duplicated(subset='original_id')) & (original_song_df.original_id!='Unknown')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge the original of each clique with cover dataframe\n",
    "covers=covers.merge(original_song_df[['clique_id','original_id']],how='left',left_on='clique_id',right_on='clique_id')\n",
    "all_duplicates=covers[covers.duplicated('shsPerf',keep=False) & covers.shsPerf!=0].sort_values('shsPerf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resolve ISSUE n°6\n",
    "\n",
    "# Function removing duplicates\n",
    "def duplicate_to_keep(dup_shsPerf) :\n",
    "    dup=covers[covers.shsPerf==dup_shsPerf]\n",
    "    dup.replace(['Unknown','Unavailable'],0)\n",
    "    count=(dup[['language','date','original_shsPerf','original_id']].iloc[:,1:] == 0).sum(axis=1).sort_values()\n",
    "    trackID_keep=covers.iloc[count.index[0]].trackID\n",
    "    \n",
    "    return trackID_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve ISSUE n°6\n",
    "dup_shsPerf=covers[covers.duplicated('shsPerf') & covers.shsPerf!=0].sort_values('shsPerf').shsPerf\n",
    "to_keep=pd.DataFrame(data=dup_shsPerf.map(duplicate_to_keep))\n",
    "to_remove=all_duplicates.merge(to_keep,how='left',left_on='trackID',right_on='shsPerf')\n",
    "to_remove=to_remove[to_remove.shsPerf_y.isnull()]\n",
    "# Remove the duplicates from the covers dataframe\n",
    "covers=covers[~covers.trackID.isin(to_remove.trackID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tracks removed after removing duplicates :  80\n"
     ]
    }
   ],
   "source": [
    "print('Number of tracks removed after removing duplicates : ', len(to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resolve ISSUE n°7\n",
    "\n",
    "# Function that merge cliques that link to the same original song\n",
    "def merge_cliques(original_id, covers, cliques):\n",
    "    cliques_list = cliques[cliques['original_id'] == original_id].index\n",
    "    c1 = cliques_list[0]\n",
    "    for c in cliques_list[1:]:\n",
    "        tracks = covers[covers['clique_id'] == c].index\n",
    "        for t in tracks:\n",
    "            covers.set_value(t, 'clique_id', c1)\n",
    "\n",
    "def merge(covers):\n",
    "    covers = covers\n",
    "    cliques = covers.groupby('clique_id').max()\n",
    "    cliques = cliques[cliques['original_id'] != \"Unknown\"]\n",
    "    dup = (cliques[cliques['original_id'].duplicated()].sort_values(\"original_id\")['original_id']).tolist()\n",
    "    for d in dup:\n",
    "        merge_cliques(d, covers, cliques)\n",
    "        \n",
    "    covers.sort_values(\"clique_id\", inplace=True)\n",
    "    covers.reset_index(inplace=True)\n",
    "    covers.drop('index', axis=1, inplace=True)\n",
    "    return covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trackID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>shsPerf</th>\n",
       "      <th>clique_id</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>language</th>\n",
       "      <th>date</th>\n",
       "      <th>original_shsPerf</th>\n",
       "      <th>original_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRGDMZP128F42BC52B</td>\n",
       "      <td>ARB1DDF1187FB4FCFB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Louis Armstrong</td>\n",
       "      <td>Stardust</td>\n",
       "      <td>Louis Armstrong &amp; His Orchestra</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>1988</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRCATYW12903D038FE</td>\n",
       "      <td>ARGJEEO1271F573FD6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Artie Shaw and his orchestra</td>\n",
       "      <td>Stardust</td>\n",
       "      <td>Artie Shaw and his orchestra</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>1988</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRVMZJZ128F4270CE4</td>\n",
       "      <td>ARY0HTV1187FB4A1B1</td>\n",
       "      <td>412972</td>\n",
       "      <td>0</td>\n",
       "      <td>Hoagy Carmichael</td>\n",
       "      <td>Star Dust</td>\n",
       "      <td>Hoagy Carmichael</td>\n",
       "      <td>English</td>\n",
       "      <td>1942</td>\n",
       "      <td>19677</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRKOINL128F42926C3</td>\n",
       "      <td>ARQ5FSZ1187B98AD74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Connee Boswell &amp; Sy Oliver Orchestra</td>\n",
       "      <td>Star Dust</td>\n",
       "      <td>Connee Boswell &amp; Sy Oliver Orchestra</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TROJZTF128F428B546</td>\n",
       "      <td>ARJN76O1187FB43C99</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ana Belén</td>\n",
       "      <td>Yo Vengo A Ofrecer Mi Corazon</td>\n",
       "      <td>Ana Belén</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TRYQEDQ128F427917C</td>\n",
       "      <td>ARS4KT21187B9B9438</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fito Paez</td>\n",
       "      <td>Yo Vengo A Ofrecer Mi Corazon</td>\n",
       "      <td>Fito Paez</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TRCKNGE128F92DA3F3</td>\n",
       "      <td>AR1CB5G1187B9AFB8E</td>\n",
       "      <td>16660</td>\n",
       "      <td>2</td>\n",
       "      <td>Electric Light Orchestra</td>\n",
       "      <td>Mr. Blue Sky</td>\n",
       "      <td>Electric Light Orchestra</td>\n",
       "      <td>English</td>\n",
       "      <td>1977</td>\n",
       "      <td>16660</td>\n",
       "      <td>16660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TRIOPLY128F423CFF3</td>\n",
       "      <td>ARKZJ301187FB521B2</td>\n",
       "      <td>551633</td>\n",
       "      <td>2</td>\n",
       "      <td>Lily Allen</td>\n",
       "      <td>Mr Blue Sky</td>\n",
       "      <td>Lily Allen</td>\n",
       "      <td>English</td>\n",
       "      <td>2009</td>\n",
       "      <td>16660</td>\n",
       "      <td>16660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TRWNDEU128F9329BF7</td>\n",
       "      <td>ARVZWQ31187B9B8946</td>\n",
       "      <td>354066</td>\n",
       "      <td>3</td>\n",
       "      <td>Liars</td>\n",
       "      <td>Mr Your On Fire Mr</td>\n",
       "      <td>Liars</td>\n",
       "      <td>English</td>\n",
       "      <td>October 1, 2002</td>\n",
       "      <td>142889</td>\n",
       "      <td>354066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TRYOPHS128F146DEFD</td>\n",
       "      <td>AR6NYHH1187B9BA128</td>\n",
       "      <td>354067</td>\n",
       "      <td>3</td>\n",
       "      <td>Yeah Yeah Yeahs</td>\n",
       "      <td>Mr. You're On Fire Mr.</td>\n",
       "      <td>Yeah Yeah Yeahs</td>\n",
       "      <td>English</td>\n",
       "      <td>June 23, 2003</td>\n",
       "      <td>354066</td>\n",
       "      <td>354066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TRQNZCE128E078A9C0</td>\n",
       "      <td>ARWILYB1187FB37DFE</td>\n",
       "      <td>52011</td>\n",
       "      <td>4</td>\n",
       "      <td>Bananarama</td>\n",
       "      <td>More_ More_ More</td>\n",
       "      <td>Bananarama</td>\n",
       "      <td>English</td>\n",
       "      <td>March 20, 1993</td>\n",
       "      <td>52010</td>\n",
       "      <td>52010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TRMBSQR128F92DF66E</td>\n",
       "      <td>ARPI2DX1187FB4CED4</td>\n",
       "      <td>52010</td>\n",
       "      <td>4</td>\n",
       "      <td>Andrea True Connection</td>\n",
       "      <td>More More More</td>\n",
       "      <td>Andrea True Connection</td>\n",
       "      <td>English</td>\n",
       "      <td>1976</td>\n",
       "      <td>52010</td>\n",
       "      <td>52010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TRTUJKS128F4262F5F</td>\n",
       "      <td>ARJA1841187FB3A029</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>David Bowie</td>\n",
       "      <td>Moonage Daydream (Live)</td>\n",
       "      <td>David Bowie</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>1996</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TRLLCAL128F428B903</td>\n",
       "      <td>ARP2RHS1187B991595</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Zen Guerrilla</td>\n",
       "      <td>Moonage Daydream</td>\n",
       "      <td>Zen Guerilla</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TREVWUZ128F4263A9B</td>\n",
       "      <td>AR9UYPT1187B9AE833</td>\n",
       "      <td>45769</td>\n",
       "      <td>6</td>\n",
       "      <td>Hear'Say</td>\n",
       "      <td>Monday Monday</td>\n",
       "      <td>Hear'Say</td>\n",
       "      <td>English</td>\n",
       "      <td>2001</td>\n",
       "      <td>9133</td>\n",
       "      <td>9133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TRGYREY128E0791913</td>\n",
       "      <td>ARQ294N1187FB53D2A</td>\n",
       "      <td>9133</td>\n",
       "      <td>6</td>\n",
       "      <td>The Mamas &amp; The Papas</td>\n",
       "      <td>Monday_ Monday</td>\n",
       "      <td>The Mamas &amp; The Papas</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TRBLBSR128F425EBFE</td>\n",
       "      <td>AR36FFP1187B9926D7</td>\n",
       "      <td>59663</td>\n",
       "      <td>7</td>\n",
       "      <td>Silicon Teens</td>\n",
       "      <td>Memphis Tennessee</td>\n",
       "      <td>Silicon Teens</td>\n",
       "      <td>English</td>\n",
       "      <td>September 1980</td>\n",
       "      <td>575</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TRLRWJK128F427D602</td>\n",
       "      <td>AR6NBDC1187FB4D96D</td>\n",
       "      <td>575</td>\n",
       "      <td>7</td>\n",
       "      <td>Chuck Berry</td>\n",
       "      <td>Memphis_ Tennessee</td>\n",
       "      <td>Chuck Berry</td>\n",
       "      <td>English</td>\n",
       "      <td>May 1959</td>\n",
       "      <td>575</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TRHFEEP12903CD3863</td>\n",
       "      <td>ARGYRE31187FB515A2</td>\n",
       "      <td>28082</td>\n",
       "      <td>7</td>\n",
       "      <td>Lonnie Mack</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>Lonnie Mack</td>\n",
       "      <td>Missing</td>\n",
       "      <td>May 1963</td>\n",
       "      <td>575</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TRWDJFG12903C94C9E</td>\n",
       "      <td>AR5ZI3K1187FB38BE0</td>\n",
       "      <td>150458</td>\n",
       "      <td>7</td>\n",
       "      <td>Jim Kweskin_ The Jug Band</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>Jim Kweskin_ The Jug Band</td>\n",
       "      <td>English</td>\n",
       "      <td>1965</td>\n",
       "      <td>575</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TRLQSWL128F932C80B</td>\n",
       "      <td>AR8JQ521187FB491AA</td>\n",
       "      <td>86557</td>\n",
       "      <td>7</td>\n",
       "      <td>Vince Taylor</td>\n",
       "      <td>Memphis Tennessee</td>\n",
       "      <td>Vince Taylor</td>\n",
       "      <td>English</td>\n",
       "      <td>February 1964</td>\n",
       "      <td>575</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TRZWJEP128F92F3178</td>\n",
       "      <td>ARUL9Y61187FB3DF6B</td>\n",
       "      <td>68280</td>\n",
       "      <td>8</td>\n",
       "      <td>Peter Yarrow</td>\n",
       "      <td>Sorrow (LP Version)</td>\n",
       "      <td>Peter_ Paul and Mary</td>\n",
       "      <td>English</td>\n",
       "      <td>May 1962</td>\n",
       "      <td>68273</td>\n",
       "      <td>68273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TRHFJNY128F428FC4B</td>\n",
       "      <td>AR5FP401187FB523C9</td>\n",
       "      <td>68275</td>\n",
       "      <td>8</td>\n",
       "      <td>Bob Dylan</td>\n",
       "      <td>Man Of Constant Sorrow</td>\n",
       "      <td>Bob Dylan</td>\n",
       "      <td>English</td>\n",
       "      <td>March 19, 1962</td>\n",
       "      <td>68273</td>\n",
       "      <td>68273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TRMNZOQ128F426254A</td>\n",
       "      <td>AR9YR2T1187FB3E9EE</td>\n",
       "      <td>69922</td>\n",
       "      <td>8</td>\n",
       "      <td>Patty Loveless</td>\n",
       "      <td>Soul Of Constant Sorrow</td>\n",
       "      <td>Patty Loveless</td>\n",
       "      <td>English</td>\n",
       "      <td>June 26, 2001</td>\n",
       "      <td>68273</td>\n",
       "      <td>68273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TRYEKWE128F1459C42</td>\n",
       "      <td>AR6AK6B1187FB48EDD</td>\n",
       "      <td>68273</td>\n",
       "      <td>8</td>\n",
       "      <td>The Soggy Bottom Boys / Dan Tyminski</td>\n",
       "      <td>I Am A Man Of Constant Sorrow</td>\n",
       "      <td>The Soggy Bottom Boys</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>68273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TRPTALT128F426F99F</td>\n",
       "      <td>ARQRDXP1187B98B9E6</td>\n",
       "      <td>305654</td>\n",
       "      <td>8</td>\n",
       "      <td>Ralph Stanley</td>\n",
       "      <td>Man Of Constant Sorrow</td>\n",
       "      <td>Ralph Stanley</td>\n",
       "      <td>English</td>\n",
       "      <td>July 8, 2003</td>\n",
       "      <td>68273</td>\n",
       "      <td>68273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TRQPBQV128F423C1E9</td>\n",
       "      <td>AREH7LP1187B9B7164</td>\n",
       "      <td>14943</td>\n",
       "      <td>9</td>\n",
       "      <td>War</td>\n",
       "      <td>Low Rider</td>\n",
       "      <td>War</td>\n",
       "      <td>English</td>\n",
       "      <td>July 1975</td>\n",
       "      <td>14943</td>\n",
       "      <td>14943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TRTTEXX128F428BE1D</td>\n",
       "      <td>ARJVH1S1187FB378C3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>JFA</td>\n",
       "      <td>Lowrider</td>\n",
       "      <td>JFA</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>14943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TRYXXWR12903CA7191</td>\n",
       "      <td>ARMBTFC1187FB56343</td>\n",
       "      <td>160334</td>\n",
       "      <td>9</td>\n",
       "      <td>Korn</td>\n",
       "      <td>Lowrider</td>\n",
       "      <td>Korn</td>\n",
       "      <td>English</td>\n",
       "      <td>October 1996</td>\n",
       "      <td>14943</td>\n",
       "      <td>14943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>TRVXQXV128F427863A</td>\n",
       "      <td>ARTO3L81187B9ACA8C</td>\n",
       "      <td>21513</td>\n",
       "      <td>10</td>\n",
       "      <td>Jeff Buckley</td>\n",
       "      <td>Lover_ You Should've Come Over</td>\n",
       "      <td>Jeff Buckley</td>\n",
       "      <td>English</td>\n",
       "      <td>August 15, 1994</td>\n",
       "      <td>21513</td>\n",
       "      <td>21513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18086</th>\n",
       "      <td>TRDTJZI128EF34E78F</td>\n",
       "      <td>AR1BI4V1187B9937A9</td>\n",
       "      <td>184203</td>\n",
       "      <td>5842</td>\n",
       "      <td>Di-rect</td>\n",
       "      <td>My Generation</td>\n",
       "      <td>Di-rect</td>\n",
       "      <td>English</td>\n",
       "      <td>November 2001</td>\n",
       "      <td>996</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18087</th>\n",
       "      <td>TRJUXRE128F92F88D4</td>\n",
       "      <td>ARHSO041187FB3D06D</td>\n",
       "      <td>33329</td>\n",
       "      <td>5842</td>\n",
       "      <td>Count Five</td>\n",
       "      <td>My Generation</td>\n",
       "      <td>Count Five</td>\n",
       "      <td>English</td>\n",
       "      <td>November 1966</td>\n",
       "      <td>996</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18088</th>\n",
       "      <td>TRKFNUK128F428A62D</td>\n",
       "      <td>ARV3CRH1187B9A1B21</td>\n",
       "      <td>14338</td>\n",
       "      <td>5842</td>\n",
       "      <td>Green Day</td>\n",
       "      <td>My Generation (Album version)</td>\n",
       "      <td>Green Day</td>\n",
       "      <td>English</td>\n",
       "      <td>December 1991</td>\n",
       "      <td>996</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18089</th>\n",
       "      <td>TROWHKP12903CCC703</td>\n",
       "      <td>ARCGI111187B9B8B99</td>\n",
       "      <td>67211</td>\n",
       "      <td>5843</td>\n",
       "      <td>Joaquin Phoenix</td>\n",
       "      <td>Cocaine Blues</td>\n",
       "      <td>Joaquin Phoenix</td>\n",
       "      <td>English</td>\n",
       "      <td>2005</td>\n",
       "      <td>9959</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18090</th>\n",
       "      <td>TRELXLZ12903CB9DC5</td>\n",
       "      <td>ARYDLXP122988F049E</td>\n",
       "      <td>62368</td>\n",
       "      <td>5843</td>\n",
       "      <td>Willie Heath Neal</td>\n",
       "      <td>Cocaine Blues</td>\n",
       "      <td>Willie Heath Neal</td>\n",
       "      <td>English</td>\n",
       "      <td>February 10, 2004</td>\n",
       "      <td>9959</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18091</th>\n",
       "      <td>TRPMGHH128E07917AC</td>\n",
       "      <td>ARF4L041187FB4D318</td>\n",
       "      <td>99651</td>\n",
       "      <td>5844</td>\n",
       "      <td>Tom Petty And The Heartbreakers</td>\n",
       "      <td>Here Comes My Girl</td>\n",
       "      <td>Tom Petty And The Heartbreakers</td>\n",
       "      <td>English</td>\n",
       "      <td>1979</td>\n",
       "      <td>99651</td>\n",
       "      <td>99651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18092</th>\n",
       "      <td>TRMTFOJ12903CAFF57</td>\n",
       "      <td>AR3KVH41187FB3F71F</td>\n",
       "      <td>99652</td>\n",
       "      <td>5844</td>\n",
       "      <td>Matthew Sweet &amp; Susanna Hoffs</td>\n",
       "      <td>Here Comes My Girl</td>\n",
       "      <td>Matthew Sweet &amp; Susanna Hoffs</td>\n",
       "      <td>English</td>\n",
       "      <td>July 21, 2009</td>\n",
       "      <td>99651</td>\n",
       "      <td>99651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18093</th>\n",
       "      <td>TRUAXLK12903CD39B5</td>\n",
       "      <td>AREYJKQ1187B9AEAB5</td>\n",
       "      <td>104087</td>\n",
       "      <td>5845</td>\n",
       "      <td>Gin Blossoms</td>\n",
       "      <td>Back Of A Car</td>\n",
       "      <td>Gin Blossoms</td>\n",
       "      <td>English</td>\n",
       "      <td>May 23, 2006</td>\n",
       "      <td>99655</td>\n",
       "      <td>99655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18094</th>\n",
       "      <td>TRNNQSH128F4245865</td>\n",
       "      <td>AR4SL791187B9890BF</td>\n",
       "      <td>0</td>\n",
       "      <td>5845</td>\n",
       "      <td>The Loud Family</td>\n",
       "      <td>Back of a Car</td>\n",
       "      <td>The Loud Family</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>1993</td>\n",
       "      <td>0</td>\n",
       "      <td>99655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18095</th>\n",
       "      <td>TRTWMBY128F9307A6D</td>\n",
       "      <td>ARSX6IX1187B9B640C</td>\n",
       "      <td>99655</td>\n",
       "      <td>5845</td>\n",
       "      <td>Big Star</td>\n",
       "      <td>Back Of A Car</td>\n",
       "      <td>Big Star</td>\n",
       "      <td>English</td>\n",
       "      <td>January 1974</td>\n",
       "      <td>99655</td>\n",
       "      <td>99655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18096</th>\n",
       "      <td>TRITRRU128F423DC76</td>\n",
       "      <td>ARJ59K61187FB5B43E</td>\n",
       "      <td>99662</td>\n",
       "      <td>5846</td>\n",
       "      <td>Afghan Whigs</td>\n",
       "      <td>Sammy (Album)</td>\n",
       "      <td>Afghan Whigs</td>\n",
       "      <td>English</td>\n",
       "      <td>1988</td>\n",
       "      <td>99662</td>\n",
       "      <td>99662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18097</th>\n",
       "      <td>TRRCCUQ128F42449F7</td>\n",
       "      <td>ARNUU2R1187B9A316C</td>\n",
       "      <td>99675</td>\n",
       "      <td>5846</td>\n",
       "      <td>Sounds Like Violence</td>\n",
       "      <td>Sammy</td>\n",
       "      <td>Sounds Like Violence</td>\n",
       "      <td>English</td>\n",
       "      <td>June 23, 2009</td>\n",
       "      <td>99662</td>\n",
       "      <td>99662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18098</th>\n",
       "      <td>TRCEMLE128F4281036</td>\n",
       "      <td>ARJ59K61187FB5B43E</td>\n",
       "      <td>99668</td>\n",
       "      <td>5847</td>\n",
       "      <td>Afghan Whigs</td>\n",
       "      <td>Going To Town</td>\n",
       "      <td>The Afghan Whigs</td>\n",
       "      <td>English</td>\n",
       "      <td>March 8, 1996</td>\n",
       "      <td>99668</td>\n",
       "      <td>99668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18099</th>\n",
       "      <td>TRCGGYF128F9338B93</td>\n",
       "      <td>ARYDZKJ1187FB540C6</td>\n",
       "      <td>99680</td>\n",
       "      <td>5847</td>\n",
       "      <td>Marshall_ Susan</td>\n",
       "      <td>Going To Town</td>\n",
       "      <td>Marshall_ Susan</td>\n",
       "      <td>English</td>\n",
       "      <td>April 28, 2009</td>\n",
       "      <td>99668</td>\n",
       "      <td>99668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18100</th>\n",
       "      <td>TRAHFYD128F426BBAD</td>\n",
       "      <td>AR94ZOI1187FB46BDA</td>\n",
       "      <td>103980</td>\n",
       "      <td>5848</td>\n",
       "      <td>Willie Dixon</td>\n",
       "      <td>The Same Thing</td>\n",
       "      <td>Willie Dixon</td>\n",
       "      <td>English</td>\n",
       "      <td>1970</td>\n",
       "      <td>9967</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18101</th>\n",
       "      <td>TRRUFPY128F4261744</td>\n",
       "      <td>ARJWDFW1241B9C69AE</td>\n",
       "      <td>118487</td>\n",
       "      <td>5848</td>\n",
       "      <td>Todd Wolfe Blues Project</td>\n",
       "      <td>Same Thing</td>\n",
       "      <td>Todd Wolfe Blues Project</td>\n",
       "      <td>English</td>\n",
       "      <td>1999</td>\n",
       "      <td>9967</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18102</th>\n",
       "      <td>TRTFWRF128F92E378E</td>\n",
       "      <td>ARX1CYE1187FB3A97B</td>\n",
       "      <td>99730</td>\n",
       "      <td>5849</td>\n",
       "      <td>Bill Anderson</td>\n",
       "      <td>The Tip Of My Fingers</td>\n",
       "      <td>Bill Anderson</td>\n",
       "      <td>English</td>\n",
       "      <td>May 9, 1960</td>\n",
       "      <td>99735</td>\n",
       "      <td>99730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18103</th>\n",
       "      <td>TRBXSHQ128F4278320</td>\n",
       "      <td>ARLM6I41187B9BA123</td>\n",
       "      <td>99731</td>\n",
       "      <td>5849</td>\n",
       "      <td>Roy Clark</td>\n",
       "      <td>The Tip Of My Fingers</td>\n",
       "      <td>Roy Clark</td>\n",
       "      <td>English</td>\n",
       "      <td>April 1963</td>\n",
       "      <td>99730</td>\n",
       "      <td>99730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18104</th>\n",
       "      <td>TRGOMBU128F42895B9</td>\n",
       "      <td>AR14CJ91187FB3A994</td>\n",
       "      <td>99733</td>\n",
       "      <td>5849</td>\n",
       "      <td>Eddy Arnold</td>\n",
       "      <td>The Tip of My Fingers</td>\n",
       "      <td>Eddy Arnold</td>\n",
       "      <td>English</td>\n",
       "      <td>June 1966</td>\n",
       "      <td>99730</td>\n",
       "      <td>99730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18105</th>\n",
       "      <td>TRMCRMX128F4267AC3</td>\n",
       "      <td>ARSPUHE1187B99DB8A</td>\n",
       "      <td>125607</td>\n",
       "      <td>5850</td>\n",
       "      <td>Frank Marino &amp; Mahogany Rush</td>\n",
       "      <td>Norwegian Wood (This Bird Has Flown)</td>\n",
       "      <td>Frank Marino &amp; Mahogany Rush</td>\n",
       "      <td>English</td>\n",
       "      <td>1979</td>\n",
       "      <td>998</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18106</th>\n",
       "      <td>TRNBSWN128F1473B31</td>\n",
       "      <td>ARLG9UJ1187B9B6C67</td>\n",
       "      <td>24567</td>\n",
       "      <td>5850</td>\n",
       "      <td>Charles River Valley Boys</td>\n",
       "      <td>Norwegian Wood (LP Version)</td>\n",
       "      <td>Charles River Valley Boys</td>\n",
       "      <td>English</td>\n",
       "      <td>1966</td>\n",
       "      <td>998</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18107</th>\n",
       "      <td>TRWAULS128F1474CBF</td>\n",
       "      <td>AR7GM0K1187B9B74B4</td>\n",
       "      <td>20304</td>\n",
       "      <td>5851</td>\n",
       "      <td>The J. Geils Band</td>\n",
       "      <td>Night Time</td>\n",
       "      <td>J. Geils Band</td>\n",
       "      <td>English</td>\n",
       "      <td>1980</td>\n",
       "      <td>9981</td>\n",
       "      <td>9981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18108</th>\n",
       "      <td>TRZVRJL128F92E2984</td>\n",
       "      <td>ARDYR3C1187FB461CE</td>\n",
       "      <td>9982</td>\n",
       "      <td>5851</td>\n",
       "      <td>George Thorogood And The Destroyers</td>\n",
       "      <td>Night Time</td>\n",
       "      <td>George Thorogood And The Destroyers</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18109</th>\n",
       "      <td>TRMQPSQ128F426C548</td>\n",
       "      <td>ARAWPR61187B9B5A4C</td>\n",
       "      <td>9981</td>\n",
       "      <td>5851</td>\n",
       "      <td>The Strangeloves</td>\n",
       "      <td>Night-Time</td>\n",
       "      <td>The Strangeloves</td>\n",
       "      <td>English</td>\n",
       "      <td>December 1965</td>\n",
       "      <td>9981</td>\n",
       "      <td>9981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18110</th>\n",
       "      <td>TRUZAVE128F426E391</td>\n",
       "      <td>AR3DLBB1187B98F3DF</td>\n",
       "      <td>106651</td>\n",
       "      <td>5852</td>\n",
       "      <td>The Statler Brothers</td>\n",
       "      <td>Oh Happy Day</td>\n",
       "      <td>The Statler Brothers</td>\n",
       "      <td>English</td>\n",
       "      <td>June 25, 1969</td>\n",
       "      <td>284962</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18111</th>\n",
       "      <td>TRDNIRY128F425A5C8</td>\n",
       "      <td>ARDRJSP126E2B3BEF8</td>\n",
       "      <td>248918</td>\n",
       "      <td>5852</td>\n",
       "      <td>Spiritualized</td>\n",
       "      <td>Oh Happy Day</td>\n",
       "      <td>Spiritualized</td>\n",
       "      <td>English</td>\n",
       "      <td>1998</td>\n",
       "      <td>284962</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18112</th>\n",
       "      <td>TRSDUWY128F92DD39A</td>\n",
       "      <td>ARR176H1187FB4CDFE</td>\n",
       "      <td>99838</td>\n",
       "      <td>5852</td>\n",
       "      <td>Edwin Hawkins Singers</td>\n",
       "      <td>Oh Happy Day</td>\n",
       "      <td>The Edwin Hawkins Singers</td>\n",
       "      <td>English</td>\n",
       "      <td>1968</td>\n",
       "      <td>284962</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18113</th>\n",
       "      <td>TRPVDKP128F934956E</td>\n",
       "      <td>ARELPXQ1187FB384FD</td>\n",
       "      <td>138704</td>\n",
       "      <td>5852</td>\n",
       "      <td>Queen Latifah</td>\n",
       "      <td>Oh Happy Day</td>\n",
       "      <td>Queen Latifah with Jubilation</td>\n",
       "      <td>English</td>\n",
       "      <td>October 6, 2009</td>\n",
       "      <td>284962</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18114</th>\n",
       "      <td>TREMVLC128F92EFA95</td>\n",
       "      <td>ARBFDJW1187B9AD27A</td>\n",
       "      <td>29576</td>\n",
       "      <td>5853</td>\n",
       "      <td>The Fabulous Thunderbirds</td>\n",
       "      <td>Tip On In</td>\n",
       "      <td>The Fabulous Thunderbirds</td>\n",
       "      <td>Missing</td>\n",
       "      <td>1981</td>\n",
       "      <td>9984</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18115</th>\n",
       "      <td>TRMTMWV128F92F6F78</td>\n",
       "      <td>ARDYR3C1187FB461CE</td>\n",
       "      <td>9986</td>\n",
       "      <td>5853</td>\n",
       "      <td>George Thorogood And The Destroyers</td>\n",
       "      <td>Tip On In</td>\n",
       "      <td>George Thorogood &amp; The Destroyers</td>\n",
       "      <td>Missing</td>\n",
       "      <td>1980</td>\n",
       "      <td>9984</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18116 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  trackID            artistID  shsPerf  clique_id  \\\n",
       "0      TRGDMZP128F42BC52B  ARB1DDF1187FB4FCFB        0          0   \n",
       "1      TRCATYW12903D038FE  ARGJEEO1271F573FD6        0          0   \n",
       "2      TRVMZJZ128F4270CE4  ARY0HTV1187FB4A1B1   412972          0   \n",
       "3      TRKOINL128F42926C3  ARQ5FSZ1187B98AD74        0          0   \n",
       "4      TROJZTF128F428B546  ARJN76O1187FB43C99        0          1   \n",
       "5      TRYQEDQ128F427917C  ARS4KT21187B9B9438        0          1   \n",
       "6      TRCKNGE128F92DA3F3  AR1CB5G1187B9AFB8E    16660          2   \n",
       "7      TRIOPLY128F423CFF3  ARKZJ301187FB521B2   551633          2   \n",
       "8      TRWNDEU128F9329BF7  ARVZWQ31187B9B8946   354066          3   \n",
       "9      TRYOPHS128F146DEFD  AR6NYHH1187B9BA128   354067          3   \n",
       "10     TRQNZCE128E078A9C0  ARWILYB1187FB37DFE    52011          4   \n",
       "11     TRMBSQR128F92DF66E  ARPI2DX1187FB4CED4    52010          4   \n",
       "12     TRTUJKS128F4262F5F  ARJA1841187FB3A029        0          5   \n",
       "13     TRLLCAL128F428B903  ARP2RHS1187B991595        0          5   \n",
       "14     TREVWUZ128F4263A9B  AR9UYPT1187B9AE833    45769          6   \n",
       "15     TRGYREY128E0791913  ARQ294N1187FB53D2A     9133          6   \n",
       "16     TRBLBSR128F425EBFE  AR36FFP1187B9926D7    59663          7   \n",
       "17     TRLRWJK128F427D602  AR6NBDC1187FB4D96D      575          7   \n",
       "18     TRHFEEP12903CD3863  ARGYRE31187FB515A2    28082          7   \n",
       "19     TRWDJFG12903C94C9E  AR5ZI3K1187FB38BE0   150458          7   \n",
       "20     TRLQSWL128F932C80B  AR8JQ521187FB491AA    86557          7   \n",
       "21     TRZWJEP128F92F3178  ARUL9Y61187FB3DF6B    68280          8   \n",
       "22     TRHFJNY128F428FC4B  AR5FP401187FB523C9    68275          8   \n",
       "23     TRMNZOQ128F426254A  AR9YR2T1187FB3E9EE    69922          8   \n",
       "24     TRYEKWE128F1459C42  AR6AK6B1187FB48EDD    68273          8   \n",
       "25     TRPTALT128F426F99F  ARQRDXP1187B98B9E6   305654          8   \n",
       "26     TRQPBQV128F423C1E9  AREH7LP1187B9B7164    14943          9   \n",
       "27     TRTTEXX128F428BE1D  ARJVH1S1187FB378C3        0          9   \n",
       "28     TRYXXWR12903CA7191  ARMBTFC1187FB56343   160334          9   \n",
       "29     TRVXQXV128F427863A  ARTO3L81187B9ACA8C    21513         10   \n",
       "...                   ...                 ...      ...        ...   \n",
       "18086  TRDTJZI128EF34E78F  AR1BI4V1187B9937A9   184203       5842   \n",
       "18087  TRJUXRE128F92F88D4  ARHSO041187FB3D06D    33329       5842   \n",
       "18088  TRKFNUK128F428A62D  ARV3CRH1187B9A1B21    14338       5842   \n",
       "18089  TROWHKP12903CCC703  ARCGI111187B9B8B99    67211       5843   \n",
       "18090  TRELXLZ12903CB9DC5  ARYDLXP122988F049E    62368       5843   \n",
       "18091  TRPMGHH128E07917AC  ARF4L041187FB4D318    99651       5844   \n",
       "18092  TRMTFOJ12903CAFF57  AR3KVH41187FB3F71F    99652       5844   \n",
       "18093  TRUAXLK12903CD39B5  AREYJKQ1187B9AEAB5   104087       5845   \n",
       "18094  TRNNQSH128F4245865  AR4SL791187B9890BF        0       5845   \n",
       "18095  TRTWMBY128F9307A6D  ARSX6IX1187B9B640C    99655       5845   \n",
       "18096  TRITRRU128F423DC76  ARJ59K61187FB5B43E    99662       5846   \n",
       "18097  TRRCCUQ128F42449F7  ARNUU2R1187B9A316C    99675       5846   \n",
       "18098  TRCEMLE128F4281036  ARJ59K61187FB5B43E    99668       5847   \n",
       "18099  TRCGGYF128F9338B93  ARYDZKJ1187FB540C6    99680       5847   \n",
       "18100  TRAHFYD128F426BBAD  AR94ZOI1187FB46BDA   103980       5848   \n",
       "18101  TRRUFPY128F4261744  ARJWDFW1241B9C69AE   118487       5848   \n",
       "18102  TRTFWRF128F92E378E  ARX1CYE1187FB3A97B    99730       5849   \n",
       "18103  TRBXSHQ128F4278320  ARLM6I41187B9BA123    99731       5849   \n",
       "18104  TRGOMBU128F42895B9  AR14CJ91187FB3A994    99733       5849   \n",
       "18105  TRMCRMX128F4267AC3  ARSPUHE1187B99DB8A   125607       5850   \n",
       "18106  TRNBSWN128F1473B31  ARLG9UJ1187B9B6C67    24567       5850   \n",
       "18107  TRWAULS128F1474CBF  AR7GM0K1187B9B74B4    20304       5851   \n",
       "18108  TRZVRJL128F92E2984  ARDYR3C1187FB461CE     9982       5851   \n",
       "18109  TRMQPSQ128F426C548  ARAWPR61187B9B5A4C     9981       5851   \n",
       "18110  TRUZAVE128F426E391  AR3DLBB1187B98F3DF   106651       5852   \n",
       "18111  TRDNIRY128F425A5C8  ARDRJSP126E2B3BEF8   248918       5852   \n",
       "18112  TRSDUWY128F92DD39A  ARR176H1187FB4CDFE    99838       5852   \n",
       "18113  TRPVDKP128F934956E  ARELPXQ1187FB384FD   138704       5852   \n",
       "18114  TREMVLC128F92EFA95  ARBFDJW1187B9AD27A    29576       5853   \n",
       "18115  TRMTMWV128F92F6F78  ARDYR3C1187FB461CE     9986       5853   \n",
       "\n",
       "                                       name  \\\n",
       "0                           Louis Armstrong   \n",
       "1              Artie Shaw and his orchestra   \n",
       "2                          Hoagy Carmichael   \n",
       "3      Connee Boswell & Sy Oliver Orchestra   \n",
       "4                                 Ana Belén   \n",
       "5                                 Fito Paez   \n",
       "6                  Electric Light Orchestra   \n",
       "7                                Lily Allen   \n",
       "8                                     Liars   \n",
       "9                           Yeah Yeah Yeahs   \n",
       "10                               Bananarama   \n",
       "11                   Andrea True Connection   \n",
       "12                              David Bowie   \n",
       "13                            Zen Guerrilla   \n",
       "14                                 Hear'Say   \n",
       "15                    The Mamas & The Papas   \n",
       "16                            Silicon Teens   \n",
       "17                              Chuck Berry   \n",
       "18                              Lonnie Mack   \n",
       "19                Jim Kweskin_ The Jug Band   \n",
       "20                             Vince Taylor   \n",
       "21                             Peter Yarrow   \n",
       "22                                Bob Dylan   \n",
       "23                           Patty Loveless   \n",
       "24     The Soggy Bottom Boys / Dan Tyminski   \n",
       "25                            Ralph Stanley   \n",
       "26                                      War   \n",
       "27                                      JFA   \n",
       "28                                     Korn   \n",
       "29                             Jeff Buckley   \n",
       "...                                     ...   \n",
       "18086                               Di-rect   \n",
       "18087                            Count Five   \n",
       "18088                             Green Day   \n",
       "18089                       Joaquin Phoenix   \n",
       "18090                     Willie Heath Neal   \n",
       "18091       Tom Petty And The Heartbreakers   \n",
       "18092         Matthew Sweet & Susanna Hoffs   \n",
       "18093                          Gin Blossoms   \n",
       "18094                       The Loud Family   \n",
       "18095                              Big Star   \n",
       "18096                          Afghan Whigs   \n",
       "18097                  Sounds Like Violence   \n",
       "18098                          Afghan Whigs   \n",
       "18099                       Marshall_ Susan   \n",
       "18100                          Willie Dixon   \n",
       "18101              Todd Wolfe Blues Project   \n",
       "18102                         Bill Anderson   \n",
       "18103                             Roy Clark   \n",
       "18104                           Eddy Arnold   \n",
       "18105          Frank Marino & Mahogany Rush   \n",
       "18106             Charles River Valley Boys   \n",
       "18107                     The J. Geils Band   \n",
       "18108   George Thorogood And The Destroyers   \n",
       "18109                      The Strangeloves   \n",
       "18110                  The Statler Brothers   \n",
       "18111                         Spiritualized   \n",
       "18112                 Edwin Hawkins Singers   \n",
       "18113                         Queen Latifah   \n",
       "18114             The Fabulous Thunderbirds   \n",
       "18115   George Thorogood And The Destroyers   \n",
       "\n",
       "                                      title  \\\n",
       "0                                  Stardust   \n",
       "1                                  Stardust   \n",
       "2                                 Star Dust   \n",
       "3                                 Star Dust   \n",
       "4             Yo Vengo A Ofrecer Mi Corazon   \n",
       "5             Yo Vengo A Ofrecer Mi Corazon   \n",
       "6                              Mr. Blue Sky   \n",
       "7                               Mr Blue Sky   \n",
       "8                        Mr Your On Fire Mr   \n",
       "9                    Mr. You're On Fire Mr.   \n",
       "10                         More_ More_ More   \n",
       "11                           More More More   \n",
       "12                  Moonage Daydream (Live)   \n",
       "13                         Moonage Daydream   \n",
       "14                            Monday Monday   \n",
       "15                           Monday_ Monday   \n",
       "16                        Memphis Tennessee   \n",
       "17                       Memphis_ Tennessee   \n",
       "18                                  Memphis   \n",
       "19                                  Memphis   \n",
       "20                        Memphis Tennessee   \n",
       "21                      Sorrow (LP Version)   \n",
       "22                   Man Of Constant Sorrow   \n",
       "23                  Soul Of Constant Sorrow   \n",
       "24            I Am A Man Of Constant Sorrow   \n",
       "25                   Man Of Constant Sorrow   \n",
       "26                                Low Rider   \n",
       "27                                 Lowrider   \n",
       "28                                 Lowrider   \n",
       "29           Lover_ You Should've Come Over   \n",
       "...                                     ...   \n",
       "18086                         My Generation   \n",
       "18087                         My Generation   \n",
       "18088         My Generation (Album version)   \n",
       "18089                         Cocaine Blues   \n",
       "18090                         Cocaine Blues   \n",
       "18091                    Here Comes My Girl   \n",
       "18092                    Here Comes My Girl   \n",
       "18093                         Back Of A Car   \n",
       "18094                         Back of a Car   \n",
       "18095                         Back Of A Car   \n",
       "18096                         Sammy (Album)   \n",
       "18097                                 Sammy   \n",
       "18098                         Going To Town   \n",
       "18099                         Going To Town   \n",
       "18100                        The Same Thing   \n",
       "18101                            Same Thing   \n",
       "18102                 The Tip Of My Fingers   \n",
       "18103                 The Tip Of My Fingers   \n",
       "18104                 The Tip of My Fingers   \n",
       "18105  Norwegian Wood (This Bird Has Flown)   \n",
       "18106           Norwegian Wood (LP Version)   \n",
       "18107                            Night Time   \n",
       "18108                            Night Time   \n",
       "18109                            Night-Time   \n",
       "18110                          Oh Happy Day   \n",
       "18111                          Oh Happy Day   \n",
       "18112                          Oh Happy Day   \n",
       "18113                          Oh Happy Day   \n",
       "18114                             Tip On In   \n",
       "18115                             Tip On In   \n",
       "\n",
       "                                     artist     language               date  \\\n",
       "0           Louis Armstrong & His Orchestra  Unavailable               1988   \n",
       "1              Artie Shaw and his orchestra  Unavailable               1988   \n",
       "2                          Hoagy Carmichael      English               1942   \n",
       "3      Connee Boswell & Sy Oliver Orchestra  Unavailable                NaN   \n",
       "4                                 Ana Belén  Unavailable               2001   \n",
       "5                                 Fito Paez  Unavailable                NaN   \n",
       "6                  Electric Light Orchestra      English               1977   \n",
       "7                                Lily Allen      English               2009   \n",
       "8                                     Liars      English    October 1, 2002   \n",
       "9                           Yeah Yeah Yeahs      English      June 23, 2003   \n",
       "10                               Bananarama      English     March 20, 1993   \n",
       "11                   Andrea True Connection      English               1976   \n",
       "12                              David Bowie  Unavailable               1996   \n",
       "13                             Zen Guerilla  Unavailable                NaN   \n",
       "14                                 Hear'Say      English               2001   \n",
       "15                    The Mamas & The Papas  Unavailable                NaN   \n",
       "16                            Silicon Teens      English     September 1980   \n",
       "17                              Chuck Berry      English           May 1959   \n",
       "18                              Lonnie Mack      Missing           May 1963   \n",
       "19                Jim Kweskin_ The Jug Band      English               1965   \n",
       "20                             Vince Taylor      English      February 1964   \n",
       "21                     Peter_ Paul and Mary      English           May 1962   \n",
       "22                                Bob Dylan      English     March 19, 1962   \n",
       "23                           Patty Loveless      English      June 26, 2001   \n",
       "24                    The Soggy Bottom Boys  Unavailable               2002   \n",
       "25                            Ralph Stanley      English       July 8, 2003   \n",
       "26                                      War      English          July 1975   \n",
       "27                                      JFA  Unavailable               2003   \n",
       "28                                     Korn      English       October 1996   \n",
       "29                             Jeff Buckley      English    August 15, 1994   \n",
       "...                                     ...          ...                ...   \n",
       "18086                               Di-rect      English      November 2001   \n",
       "18087                            Count Five      English      November 1966   \n",
       "18088                             Green Day      English      December 1991   \n",
       "18089                       Joaquin Phoenix      English               2005   \n",
       "18090                     Willie Heath Neal      English  February 10, 2004   \n",
       "18091       Tom Petty And The Heartbreakers      English               1979   \n",
       "18092         Matthew Sweet & Susanna Hoffs      English      July 21, 2009   \n",
       "18093                          Gin Blossoms      English       May 23, 2006   \n",
       "18094                       The Loud Family  Unavailable               1993   \n",
       "18095                              Big Star      English       January 1974   \n",
       "18096                          Afghan Whigs      English               1988   \n",
       "18097                  Sounds Like Violence      English      June 23, 2009   \n",
       "18098                      The Afghan Whigs      English      March 8, 1996   \n",
       "18099                       Marshall_ Susan      English     April 28, 2009   \n",
       "18100                          Willie Dixon      English               1970   \n",
       "18101              Todd Wolfe Blues Project      English               1999   \n",
       "18102                         Bill Anderson      English        May 9, 1960   \n",
       "18103                             Roy Clark      English         April 1963   \n",
       "18104                           Eddy Arnold      English          June 1966   \n",
       "18105          Frank Marino & Mahogany Rush      English               1979   \n",
       "18106             Charles River Valley Boys      English               1966   \n",
       "18107                         J. Geils Band      English               1980   \n",
       "18108   George Thorogood And The Destroyers      Missing                NaN   \n",
       "18109                      The Strangeloves      English      December 1965   \n",
       "18110                  The Statler Brothers      English      June 25, 1969   \n",
       "18111                         Spiritualized      English               1998   \n",
       "18112             The Edwin Hawkins Singers      English               1968   \n",
       "18113         Queen Latifah with Jubilation      English    October 6, 2009   \n",
       "18114             The Fabulous Thunderbirds      Missing               1981   \n",
       "18115     George Thorogood & The Destroyers      Missing               1980   \n",
       "\n",
       "       original_shsPerf original_id  \n",
       "0                     0     Unknown  \n",
       "1                     0     Unknown  \n",
       "2                 19677     Unknown  \n",
       "3                     0     Unknown  \n",
       "4                     0     Unknown  \n",
       "5                     0     Unknown  \n",
       "6                 16660       16660  \n",
       "7                 16660       16660  \n",
       "8                142889      354066  \n",
       "9                354066      354066  \n",
       "10                52010       52010  \n",
       "11                52010       52010  \n",
       "12                    0     Unknown  \n",
       "13                    0     Unknown  \n",
       "14                 9133        9133  \n",
       "15                    0        9133  \n",
       "16                  575         575  \n",
       "17                  575         575  \n",
       "18                  575         575  \n",
       "19                  575         575  \n",
       "20                  575         575  \n",
       "21                68273       68273  \n",
       "22                68273       68273  \n",
       "23                68273       68273  \n",
       "24                    0       68273  \n",
       "25                68273       68273  \n",
       "26                14943       14943  \n",
       "27                    0       14943  \n",
       "28                14943       14943  \n",
       "29                21513       21513  \n",
       "...                 ...         ...  \n",
       "18086               996         996  \n",
       "18087               996         996  \n",
       "18088               996         996  \n",
       "18089              9959     Unknown  \n",
       "18090              9959     Unknown  \n",
       "18091             99651       99651  \n",
       "18092             99651       99651  \n",
       "18093             99655       99655  \n",
       "18094                 0       99655  \n",
       "18095             99655       99655  \n",
       "18096             99662       99662  \n",
       "18097             99662       99662  \n",
       "18098             99668       99668  \n",
       "18099             99668       99668  \n",
       "18100              9967     Unknown  \n",
       "18101              9967     Unknown  \n",
       "18102             99735       99730  \n",
       "18103             99730       99730  \n",
       "18104             99730       99730  \n",
       "18105               998     Unknown  \n",
       "18106               998     Unknown  \n",
       "18107              9981        9981  \n",
       "18108                 0        9981  \n",
       "18109              9981        9981  \n",
       "18110            284962     Unknown  \n",
       "18111            284962     Unknown  \n",
       "18112            284962     Unknown  \n",
       "18113            284962     Unknown  \n",
       "18114              9984     Unknown  \n",
       "18115              9984     Unknown  \n",
       "\n",
       "[18116 rows x 11 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge(covers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(covers,open('data/covers_4.p','wb'))\n",
    "covers=pickle.load(open(\"data/covers_4.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tracks contained in our dataset after data wrangling :  18116\n"
     ]
    }
   ],
   "source": [
    "# Resolve ISSUE n°8\n",
    "\n",
    "# Find cliques where original_id='Unknown' and where there is at least one Nan year\n",
    "nan_years=covers[covers.original_id=='Unknown'].groupby(['clique_id','date'],as_index=False)['clique_id'].agg({'freq':'count'})\n",
    "covers=covers[~covers.clique_id.isin(nan_years[nan_years.date=='nan'].clique_id)]\n",
    "\n",
    "print('Number of tracks contained in our dataset after data wrangling : ', len(covers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The years recovered from the dataset are floats, so we need to strip their decimal part,\n",
    "# and then we can take the last 4 characters on any date string to get the year.\n",
    "covers[\"year\"] = covers[\"date\"].astype(str).replace(\"\\.0\",\"\",regex=True).str[-4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the result of two algorithms to find the original song : one using the information found via web-scrapping (**find_original_track()**) and the other one using the released dates (taking the earlier song)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In each clique, we rank the versions by year to find the original\n",
    "covers[\"rank\"] = covers.groupby(\"clique_id\")[\"year\"].rank(method=\"dense\",ascending=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for each algorithm where the resulting original track is named \"original\"  \n",
    "original_song=pd.DataFrame(covers.original_id.unique())\n",
    "original_song['first_algo_result']='original'\n",
    "original_song.columns=['original_id','first_algo_result']\n",
    "original_song.set_index('original_id',inplace=True)\n",
    "covers=covers.merge(original_song,how='left',left_on='shsPerf',right_index=True)\n",
    "year_nan=covers.groupby(['clique_id','year'],as_index=False)['clique_id'].agg({'freq':'count'})\n",
    "year_nan[year_nan.year=='nan'].clique_id.head()\n",
    "second_algo=covers[~covers.clique_id.isin(year_nan[year_nan.year=='nan'].clique_id)]\n",
    "covers['second_algo_result']= np.where(((covers['rank']==1) & (covers.clique_id.isin(second_algo.clique_id))) , 'original', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the missing dates and year to np.nan\n",
    "covers.loc[(covers.year.str.contains(\"nan\")) | (covers.year==\"\"),\"year\"]=np.nan\n",
    "covers.loc[(covers.date.str.contains(\"nan\")) | (covers.date==\"\"),\"year\"]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(covers,open('data/covers_5.p','wb'))\n",
    "covers=pickle.load(open(\"data/covers_5.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Addition of the genre for each track and the artist location\n",
    "\n",
    "PARTIE DE LOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the artist location\n",
    "artist_location=pickle.load(open(\"data/artists_location.p\",\"rb\"))\n",
    "genres=pickle.load(open(\"data/genres_shs.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artist_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the genres and the artist location to the covers dataframe\n",
    "covers=covers.merge(artist_location,how='left',left_on='name',right_on='name')\n",
    "covers=covers.merge(genres[['genre']],how='left',left_on='trackID',right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop useless columns\n",
    "covers.drop(['original_shsPerf','original_id','first_algo_result','second_algo_result','artist'],axis=1,inplace=True)\n",
    "covers.rename(columns = {'name':'artist','final_original':'status'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create multilevel index with clique_id and track_id\n",
    "covers.set_index(['clique_id','trackID'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Access to files (tempo / dancability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We open the first file of the subset, to check what the HDF5 keys are and then we read each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore(\"data/MillionSongSubset/data/A/A/A/TRAAAAW128F429D538.h5\") as hdf:\n",
    "    print(hdf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.read_hdf(\"data/MillionSongSubset/data/A/A/A/TRAAAAW128F429D538.h5\",\"/analysis/songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.read_hdf(\"data/MillionSongSubset/data/A/A/A/TRAAAAW128F429D538.h5\",\"/metadata/songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.read_hdf(\"data/MillionSongSubset/data/A/A/A/TRAAAAW128F429D538.h5\",\"/musicbrainz/songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to extract <tt>tempo</tt> and <tt>song_hotttnesss</tt>, here is an example of how to do that on the subset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tempo = []\n",
    "hotness = []\n",
    "\n",
    "files = glob.glob(\"data/MillionSongSubset/data/A\" + \"/[A-Z]/[A-Z]/*\")\n",
    "for f in files:\n",
    "    tempo.append(pd.read_hdf(f,\"/analysis/songs\")[\"tempo\"][0])\n",
    "    hotness.append(pd.read_hdf(f,\"/metadata/songs\")[\"song_hotttnesss\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tempo = np.asarray(tempo)\n",
    "hotness = np.asarray(hotness)\n",
    "print(tempo)\n",
    "print(hotness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of tracks =\", len(files))\n",
    "print(\"with missing hotness values =\", np.sum(np.isnan(hotness)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already got 3268 unknown hotness values and we only tested on a subset of 7620 song, so we can expect to have that information for only a little over half of our final dataset. Maybe we won't use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the files are accessible on the cluster, we will have to go through our SHS dataset and get those attributes for each track_id.\n",
    "We will do so in the following way : (the paths are just examples on the subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tempo = []\n",
    "hotness = []\n",
    "\n",
    "my_file = Path(\"/path/to/file\")\n",
    "for track in covers[\"trackID\"]:\n",
    "    folder1 = track[2]\n",
    "    folder2 = track[3]\n",
    "    folder3 = track[4]\n",
    "    folder_path = \"data/MillionSongSubset/data/\" + folder1 + \"/\" + folder2 + \"/\" + folder3 + \"/\"\n",
    "    track_path = folder_path + track + \".h5\"\n",
    "    if Path(track_path).exists(): #to delete later\n",
    "        tempo.append(pd.read_hdf(track_path,\"/analysis/songs\")[\"tempo\"][0])\n",
    "        hotness.append(pd.read_hdf(track_path,\"/metadata/songs\")[\"song_hotttnesss\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(tempo))\n",
    "print(np.sum(~np.isnan(hotness)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, we only found 204 of those tracks in the subset and 128 of them have a hotness value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Determine artist location for spatial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load Additional files\n",
    "#unique_artists=pd.read_csv('data/AdditionalFiles/unique_artists.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['artistID','artistMID','randomTrack','name'])\n",
    "unique_artists=pd.read_csv('data/AdditionalFiles/unique_artists.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['artistID','artistMID','randomTrack','name'])\n",
    "artist_location=pd.read_csv('data/AdditionalFiles/artist_location.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['artistID','lat','long','name','location'])\n",
    "artist_location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load a subset of Second Hand Song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_shs_files(pathToFile):\n",
    "    f = open(pathToFile)\n",
    "    s = StringIO()\n",
    "    cur_ID = None\n",
    "    for ln in f:\n",
    "        if not ln.strip():\n",
    "                continue\n",
    "        if ln.startswith('%'):\n",
    "                cur_ID = ln.replace('\\n','<SEP>',1)\n",
    "                continue\n",
    "        if cur_ID is None:\n",
    "                print ('NO ID found')\n",
    "                sys.exit(1)\n",
    "        s.write(cur_ID + ln)\n",
    "    s.seek(0)\n",
    "    df = pd.read_csv(s,delimiter='<SEP>',engine='python',header=None,names=['shsID','trackID','artistID','shsPerf'])\n",
    "    return df[['trackID', 'artistID', 'shsPerf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the artists' names using the unique_artists.txt file and we assign a location for each track using the artist_location.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_location(x) : \n",
    "    if x in artist_location.index:\n",
    "        return artist_location.get_value(x, 'location')\n",
    "    else : \n",
    "        return np.nan\n",
    "    \n",
    "data=read_shs_files('data/SHS_testset.txt')\n",
    "data['artist'] = data['artistID'].map(lambda x : unique_artists.get_value(x, 'name'))\n",
    "data['location'] = data['artistID'].map(lambda x : get_location(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the function finding the country for each location. In order to do that we wille use three different python packages : pycountry, us, and geopy, as geopy.geocoders does not support too much requests. \n",
    "\n",
    "- First, we will use the pycountry package to extract countries if location contains one. \n",
    "\n",
    "\n",
    "- If we didn't match any country in pycountry, we will use the us package to check if a us state is present in the location. From the data, we have observed that if the location refer to a us state, the location is either only defined by the state, or the state is the last element of the location.\n",
    "\n",
    "\n",
    "- If the two precedent methods does not succeed, we will use the geopy.geocoders package, using Nominatim( ).\n",
    "\n",
    "\n",
    "- We will manually define countries for some location as they are sometimes mispelled, troncated or refer to a website link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geolocator = Nominatim()\n",
    "\n",
    "def get_country(x):\n",
    "    if x == np.nan:\n",
    "        return x\n",
    "    x = x.replace(\"-\", \",\")\n",
    "    for c in pycountry.countries:\n",
    "        if \"England\" in x or \"UK\" in x: \n",
    "            return \"United Kingdom\"\n",
    "        elif c.name.lower() in x.lower():\n",
    "            return c.name\n",
    "    refactorlast = x.split(\",\")[-1].replace(\" \", \"\")\n",
    "    refactorfirst = x.split(\",\")[0]\n",
    "    usstatelast = us.states.lookup(refactorlast)\n",
    "    usstatefirst = us.states.lookup(refactorfirst)\n",
    "    if usstatelast != None or usstatefirst != None:\n",
    "        return \"United State of America\"\n",
    "    elif x == \"Swingtown\":\n",
    "        return \"United State of America\"\n",
    "    elif x == \"<a href=\\\"http://billyidol.net\\\" onmousedown='UntrustedLink.bootstrap($(this), \\\"fc44f8f60d13ab68c56b3c6709c6d670\\\", event)' target=\\\"_blank\\\" rel=\\\"nofollow\\\">http://billyidol.net</a>\":\n",
    "        return \"United Kingdom\"\n",
    "    elif x == \"Lennox Castle, Glasgow\" or x == \"Knowle West, Bristol, Avon, Engla\"\\\n",
    "        or x == \"Goldsmith's College, Lewisham, Lo\" or x == \"Julian Lennon&#039;s Official Facebook Music Page\"\\\n",
    "        or x == \"Sydney, Moscow, Pressburg\" or x == \"Penarth, Wales to Los Angeles\" or x == \"Leicester, Leicestershire, Englan\":\n",
    "        return \"United Kingdom\"\n",
    "    elif x == \"Vancouver, British Columbia, Cana\":\n",
    "        return \"Canada\"\n",
    "    elif x == \"Washington DC\" or x == \"Philladelphia\" or \"New Jersey\" in x:\n",
    "        return \"United State of America\"\n",
    "    elif \"Czechoslovakia\" in x :\n",
    "        return \"Česko\"\n",
    "    elif x == \"Jaded Heart Town\":\n",
    "        return \"Germany\"\n",
    "    elif x == \"RU\" or x == \"Russia\":\n",
    "        return \"Russia\"\n",
    "    else :\n",
    "        location = geolocator.geocode(x, timeout=None)\n",
    "        return location.address.split(\",\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data['country'] = data['location'].map(lambda x : get_country(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only problem with geopy is that it returns a country in its native language. To uniform our data, we create a function that translates manually the countries in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename(x):\n",
    "    if \"België - Belgique - Belgien\" in x:\n",
    "        return \"Belgium\"\n",
    "    elif \"Brasil\" in x:\n",
    "        return \"Brazil\"\n",
    "    elif \"United State\" in x:\n",
    "        return \"United States of America\"\n",
    "    elif \"Italia\" in x:\n",
    "        return \"Italy\"\n",
    "    elif \"Norge\" in x:\n",
    "        return \"Norway\"\n",
    "    elif \"España\" in x:\n",
    "        return \"Spain\"\n",
    "    elif \"Nederland\" in x :\n",
    "        return \"Netherlands\"\n",
    "    elif \"Suomi\" in x :\n",
    "        return \"Finland\"\n",
    "    elif \"Sverige\" in x :\n",
    "        return \"Sweden\"\n",
    "    elif \"UK\" in x :\n",
    "        return \"United Kingdom\"\n",
    "    elif x[0] == \" \":\n",
    "        return x[1:]\n",
    "    else : \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data['country'] = data['country'].map(lambda x : rename(x))\n",
    "#pickle.dump(data, open( \"data.p\", \"wb\" ) )\n",
    "data_country = pickle.load(open(\"data/data_country.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Addition of the genre for each track (Use of LastFM dataset and external website for genre listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the genre of a song, we will use the LastFM dataset that contains a list a tags for each song.\n",
    "Since the dataset is from the MillionSongDataset, we will not use all of the available tracks from LastFM but, but only the ones contained in the SecondHandSong dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the files if they are in the SecondHandSong dataset and create the dataframe\n",
    "covers_df = pickle.load(open(\"data/covers.p\",\"rb\"))\n",
    "list_tracks = covers_df.trackID\n",
    "test_path = \"../../lastfm_test\"\n",
    "train_path = \"../../lastfm_train\"\n",
    "\n",
    "genre_df = pd.DataFrame()\n",
    "def create_dataFrame(genre_df):\n",
    "    for track in list_tracks:\n",
    "        folder1 = track[2]\n",
    "        folder2 = track[3]\n",
    "        folder3 = track[4]\n",
    "        folder_path = \"/\" + folder1 + \"/\" + folder2 + \"/\" + folder3 + \"/\"\n",
    "        track_path = folder_path + track + \".json\"\n",
    "        if glob.glob(train_path + track_path) != []:\n",
    "                genre_df = genre_df.append(pd.DataFrame.from_dict(json.load(open(train_path + track_path)), orient=\"index\").transpose())\n",
    "        elif glob.glob(test_path + folder_path + track) != []:\n",
    "                genre_df = genre_df.append(pd.DataFrame.from_dict(json.load(open(test_path + track_path)), orient=\"index\").transpose())\n",
    "    genre_df = genre_df.reset_index()\n",
    "    return genre_df\n",
    "\n",
    "#tracks_with_tags = create_dataFrame(genre_df)\n",
    "tracks_with_tags = pickle.load(open(\"tracks_with_tags\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now list the unique tags in the resulting dataframe. Due to a time limit for the computation of the matching, we will first test on a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = list()\n",
    "for i in range (0,1000):\n",
    "    tags = tags + tracks_with_tags.tags[i]\n",
    "    \n",
    "tags = np.unique(tags).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of tags contains useless information, thus we first proceed to a pre-cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_tags = {}\n",
    "def clean_tag(x):\n",
    "    clean = x.replace(\"ooo\", \"\")\n",
    "    clean = clean.replace(\"-o\", \"\")\n",
    "    clean = clean.replace(\"o-\", \"\")\n",
    "    clean = clean.replace(\"- \", \"\")\n",
    "    clean = clean.replace(\"-\", \"\")\n",
    "    clean_tags[x] = clean\n",
    "for t in tags:\n",
    "    clean_tag(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order assign a genre to each song, we will use their different tags and try to match it with a list of genre obtained by webscrapping the http://www.musicgenreslist.com website. For more details on the webscrapping see the notebook Genre Webscrapping.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_genres = pickle.load(open(\"data/map_genres\", \"rb\"))\n",
    "all_genres = pickle.load(open(\"data/all_genres.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the Sequence Matcher package to match tags to the web-scrapped genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.80\n",
    "def match_genres():\n",
    "    i = 0\n",
    "    genre_map = {}\n",
    "    no_match = list()\n",
    "    for ind in range(0,len(tags)):\n",
    "        name1 = tags[ind]\n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "        if clean_tags[name1] == \"\":\n",
    "            genre_map[name1] = np.nan\n",
    "        best_ratio = 0\n",
    "        match = \"\"\n",
    "        for name2 in map_genres.keys():\n",
    "            if name2.lower() in name1.lower():\n",
    "                for subgenre in map_genres[name2]:\n",
    "                    ratio = SequenceMatcher(None,name1.lower(),name2.lower()).ratio()\n",
    "                    if ratio > best_ratio:       # we find the maximum similarity\n",
    "                        best_ratio = ratio\n",
    "                        match = name2\n",
    "                if (best_ratio > threshold):     # if it's superior to our threshold we add that couple to the mapping\n",
    "                    genre_map[name1] = match\n",
    "                else:\n",
    "                    genre_map[name1] = name2\n",
    "        if match == \"\":\n",
    "            for subgenre in all_genres:\n",
    "                ratio = SequenceMatcher(None,name1.lower(),name2.lower()).ratio()\n",
    "                if ratio > best_ratio:       # we find the maximum similarity\n",
    "                    best_ratio = ratio\n",
    "                    match = name2\n",
    "            if (best_ratio > threshold):     # if it's superior to our threshold we add that couple to the mapping\n",
    "                genre_map[name1] = match\n",
    "            else :\n",
    "                genre_map[name1] = np.nan\n",
    "        i = i+1\n",
    "    return (genre_map, no_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
