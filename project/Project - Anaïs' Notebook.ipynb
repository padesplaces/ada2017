{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT - *My Way* of seeing music covers\n",
    "#### Pierre-Antoine Desplaces, Ana√Øs Ladoy, Lou Richard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from io import StringIO\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data organisation \n",
    "- All the additional files were downloaded from the cluster giving all the metadata of the Million Songs dataset. They will help to elaborate a plan and a script will then search more information about a specific track (h5 files in the cluster) maybe using cluster cpu. The path to access to a track in the cluster is for example million-songs/data/A/A/A (with the 3 letters at the end being the 3rd, 4th and 5th letter on the track id).\n",
    "- The music covers will be detected using another dataset (SecondHandSongs), we have the choice to use the downloadable dataset containing 18,196 tracks (all with a connection to the MSD dataset), or to web-scrapp the SHS website (https://secondhandsongs.com/) where we have much more information (522 436 covers) but not necessarly connected to our MSD dataset. The SHS API is RESTful (return a JSON object) and we are limited to 100 requests per minute and 1000 requestion per hour but we can contact them to remove limitation.\n",
    "- Some artist are geolocalised (30% of the MSD total artists) on the artist_location dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Additional files\n",
    "tracks_per_year=pd.read_csv('data/AdditionalFiles/tracks_per_year.txt',delimiter='<SEP>',engine='python',header=None,index_col=1,names=['year','trackID','artist','title'])\n",
    "unique_tracks=pd.read_csv('data/AdditionalFiles/unique_tracks.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['trackID','songID','artist','title'])\n",
    "unique_artists=pd.read_csv('data/AdditionalFiles/unique_artists.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['artistID','artistMID','randomTrack','name'])\n",
    "artist_location=pd.read_csv('data/AdditionalFiles/artist_location.txt',delimiter='<SEP>',engine='python',header=None,index_col=0,names=['artistID','lat','long','name','location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if indexes is unique and print the number of elements for each dataframe\n",
    "print('Dataframe (Unique index, Number of elements)')\n",
    "print('tracks_per_year ',(tracks_per_year.index.is_unique,tracks_per_year.shape[0]))\n",
    "print('unique_tracks ',(unique_tracks.index.is_unique,unique_tracks.shape[0]))\n",
    "print('unique_artists ',(unique_artists.index.is_unique,unique_artists.shape[0]))\n",
    "print('artist_location ',(artist_location.index.is_unique,artist_location.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_per_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tracks.artist.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_artists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_artists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_shs_files(pathToFile):\n",
    "    f = open(pathToFile)\n",
    "    s = StringIO()\n",
    "    cur_ID = None\n",
    "    for ln in f:\n",
    "        if not ln.strip():\n",
    "                continue\n",
    "        if ln.startswith('%'):\n",
    "                cur_ID = ln.replace('\\n','<SEP>',1)\n",
    "                continue\n",
    "        if cur_ID is None:\n",
    "                print ('NO ID found')\n",
    "                sys.exit(1)\n",
    "        s.write(cur_ID + ln)\n",
    "    s.seek(0)\n",
    "    df = pd.read_csv(s,delimiter='<SEP>',engine='python',header=None,names=['shsID','trackID','artistID','shsPerf'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the two SHS datasets (SHS data splitted in a train and test set to use for ML if wanted)\n",
    "SHS_testset=read_shs_files('data/SHS_testset.txt')\n",
    "SHS_trainset=read_shs_files('data/SHS_trainset.txt')\n",
    "covers=pd.concat([SHS_testset,SHS_trainset])\n",
    "covers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covers.shsID=covers.shsID.str.strip('%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert shsID to clique id (first convert to category and get a code)\n",
    "covers=covers.assign(clique_id=(covers.shsID.astype('category')).cat.codes)\n",
    "#Remove the shsID and the shsPerf columns (useless)\n",
    "covers.drop('shsID',axis=1,inplace=True)\n",
    "#covers.drop('shsPerf',axis=1,inplace=True)\n",
    "#Merge with unique_artists dataframe to find the artist name for each track (no taking consideration of featuring since we take only the name of the artist assigned with the track)\n",
    "covers=covers.merge(unique_artists[['name']],how='left',left_on='artistID',right_index=True)\n",
    "#Take the clique id we defined as id of the dataframe (not unique index for now)\n",
    "#covers.set_index('id',inplace=True)\n",
    "#covers.sort_index(inplace=True)\n",
    "#Merge with unique_tracks dataframe to find the track name\n",
    "covers=covers.merge(unique_tracks[['title']],how='left',left_on='trackID',right_index=True)\n",
    "#Merge with tracks_per_year dataframe to find the year of each track\n",
    "covers=covers.merge(tracks_per_year[['year']],how='left',left_on='trackID',right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of cliques :', max(covers.index)+1) #Number of cliques (+1 because id starts at 0)\n",
    "print('Number of unique tracks :', len(covers.trackID.unique())) \n",
    "print('Number of unique artists :', len(covers.artistID.unique()))\n",
    "print('Number of missing trackID :', len(covers[covers.trackID.isnull()]))\n",
    "print('Number of missing artistID :', len(covers[covers.artistID.isnull()]))\n",
    "print('Number of missing years :', len(covers[covers.year.isnull()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covers=covers.sort_values(['clique_id', 'year'], ascending=[True, True]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covers.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covers[(covers.year.isnull()) & (covers.shsPerf <0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of missing years with valid shsPerf (API request on the performance page) :',len(covers[(covers.year.isnull()) & (covers.shsPerf != -1)]))\n",
    "print('Number of missing years with invalid shsPerf (API request on the search page to find shsPerf) :',len(covers[(covers.year.isnull())])-len(covers[(covers.year.isnull()) & (covers.shsPerf != -1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the missing years in order to rank the cover songs for each clique and thus, find the original song and the following covers. Since year isn't necessarly sufficient informations to discriminate the songs (cover appears sometimes in the same year than the original one), it will be better to have the entire released date for ALL the tracks if the information is available in the SHS website.\n",
    "\n",
    "Need the shsPerf to access to the song page in SHS website, where we can find informations about the language and the released date of the song. In the dataset, negative values of shsPerf are considered as missing values.\n",
    "\n",
    "Two ways of doing it :\n",
    "- For valid SHS performance ID, access to the performance page (e.g. 'https://secondhandsongs.com/performance/1983') and web-scrapping of the Language and Released date informations using the perfInfo() function.\n",
    "- For invalid SHS performance ID, API request to the search page (e.g. 'https://secondhandsongs.com/search/performance?title=blackbird&performer=beatles'), extract the perf ID with the find_PerfID() and then use the perfInfo() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the order of songs for each clique\n",
    "#covers['rank']=covers.groupby('clique_id')['year'].rank(method='dense',ascending=True).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#covers.set_index('clique_id',inplace=True)\n",
    "#covers.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle :\n",
    "- Find the missing years using API request to SHS website (if no informations about the year on the website, find another solution)\n",
    "- Find a way to detect the original song if the first cover is the same year? (also the case where two covers are made during the same year and then have the same rank.. problem with multi-index after because not unique)\n",
    "- API request to SHS website for the location and the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API request to SHS website for the page of a specific performance (defined as shsPerf) to extract Language and Date\n",
    "def perfInfo_SHS(shsPerf):\n",
    "    if shsPerf<0:\n",
    "        perfLanguage='Unavailable'\n",
    "        perfDate='Unavailable'\n",
    "    elif shsPerf>0:\n",
    "        r = requests.get('https://secondhandsongs.com/performance/'+str(shsPerf))\n",
    "        print(r.status_code)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        perfMeta=soup.find_all('dl')[0] \n",
    "        perfLanguage=perfMeta.find('dd',attrs={'itemprop':'inLanguage'})\n",
    "        if perfLanguage is None :\n",
    "            perfLanguage='Missing'\n",
    "        else :\n",
    "            perfLanguage=perfLanguage.text\n",
    "\n",
    "        perfDate=perfMeta.find('div',attrs={'class':'media-body'})\n",
    "        if perfDate is None :\n",
    "            perfDate='Missing'\n",
    "        else :\n",
    "            perfDate=perfDate.find('p').text.split('\\n')[2].strip(' ')\n",
    "\n",
    "    return perfLanguage,perfDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#language=perfInfo_SHS(-1)[0]\n",
    "#date=perfInfo_SHS(covers.iloc[18193].shsPerf)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=covers.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test['language'], \\\n",
    "#test['date'] = zip(*test.shsPerf.map(perfInfo_SHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web-scraping - Part of the dataframe each hour (1000 requests by hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part=covers[0:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part['language'], \\\n",
    "#part['date'] = zip(*part.shsPerf.map(perfInfo_SHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part.to_csv('data/partSHS_webscraping/part_0_800.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
