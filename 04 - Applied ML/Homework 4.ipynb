{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "#### Pierre-Antoine Desplaces, Anaïs Ladoy, Lou Richard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> <font color='purple'>Question 1 - Propensity score matching</font> </b>\n",
    "In this exercise, you will apply [propensity score matching](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf), which we discussed in lecture 5 (\"Observational studies\"), in order to draw conclusions from an observational study.\n",
    "\n",
    "We will work with a by-now classic dataset from Robert LaLonde's study \"[Evaluating the Econometric Evaluations of Training Programs](http://people.hbs.edu/nashraf/LaLonde_1986.pdf)\" (1986).\n",
    "The study investigated the effect of a job training program (\"National Supported Work Demonstration\") on the real earnings of an individual, a couple of years after completion of the program.\n",
    "Your task is to determine the effectiveness of the \"treatment\" represented by the job training program.\n",
    "\n",
    "#### Dataset description\n",
    "\n",
    "- `treat`: 1 if the subject participated in the job training program, 0 otherwise\n",
    "- `age`: the subject's age\n",
    "- `educ`: years of education\n",
    "- `race`: categorical variable with three possible values: Black, Hispanic, or White\n",
    "- `married`: 1 if the subject was married at the time of the training program, 0 otherwise\n",
    "- `nodegree`: 1 if the subject has earned no school degree, 0 otherwise\n",
    "- `re74`: real earnings in 1974 (pre-treatment)\n",
    "- `re75`: real earnings in 1975 (pre-treatment)\n",
    "- `re78`: real earnings in 1978 (outcome)\n",
    "\n",
    "If you want to brush up your knowledge on propensity scores and observational studies, we highly recommend Rosenbaum's excellent book on the [\"Design of Observational Studies\"](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf). Even just reading the first chapter (18 pages) will help you a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <font color='purple'>1.1 - A naive analysis</font> </b>\n",
    "<b> Compare the distribution of the outcome variable (`re78`) between the two groups, using plots and numbers.\n",
    "To summarize and compare the distributions, you may use the techniques we discussed in lectures 4 (\"Read the stats carefully\") and 6 (\"Data visualization\").\n",
    "\n",
    "What might a naive \"researcher\" conclude from this superficial analysis? <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "lalonde_data=pd.read_csv('lalonde.csv',index_col=0)\n",
    "lalonde_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two subsets of the dataset (one for each group)\n",
    "data_control=lalonde_data[lalonde_data.treat==0]\n",
    "data_treat=lalonde_data[lalonde_data.treat==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the respective distribution of the outcome variable for the two groups with an histogram \n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "# Histrogram for the control group\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(data_control.re78,color='#00ccff')\n",
    "plt.title('Distribution of earning for the group without job training')\n",
    "plt.xlabel('real earning in 1978 [$]')\n",
    "plt.ylabel('frequency')\n",
    "# Histrogram for the treated group\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(data_treat.re78,color='#00ccff')\n",
    "plt.title('Distribution of earning for the group with job training')\n",
    "plt.xlabel('real earning in 1978 [$]')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can see that the two distributions are highly skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot([data_treat.re78, data_control.re78])\n",
    "ax.set_title('Real earning in 1978')\n",
    "ax.set_xticklabels(['With job training','Without job training'])\n",
    "ax.set_ylabel('earning [$]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ks_2samp(data_treat.re78, data_control.re78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another test to compare two samples is the two-samples Kolmogorov–Smirnov test that compute the maximum distance between the two empirical functions of two samples.  \n",
    "The null hypothesis of the Kolmogorov–Smirnov statistic is that the samples are drawn from the same distribution.  \n",
    "Comparing the distribution of the outcome variable for the group with and without job training program, we find a small K-S statistic value and a high p-value (15.27%), then we cannot reject the null hypothesis that the distributions are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decriptive statistics for the distribution of the outcome variable\n",
    "pd.DataFrame({'Without job training' :data_control.re78.describe(), 'With job training':data_treat.re78.describe()}).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <font color='purple'>1.2 - A closer look at the data</font> </b>\n",
    "<b> You're not naive, of course (and even if you are, you've learned certain things in ADA), so you aren't content with a superficial analysis such as the above.\n",
    "You're aware of the dangers of observational studies, so you take a closer look at the data before jumping to conclusions.\n",
    "\n",
    "For each feature in the dataset, compare its distribution in the treated group with its distribution in the control group, using plots and numbers.\n",
    "As above, you may use the techniques we discussed in class for summarizing and comparing the distributions.\n",
    "\n",
    "What do you observe?\n",
    "Describe what your observations mean for the conclusions drawn by the naive \"researcher\" from his superficial analysis. <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Range of values for each feature in the dataset:')\n",
    "for column in lalonde_data[lalonde_data.columns[1:]] :\n",
    "    print('{} : {} - {}'.format(column,min(lalonde_data[column]),max(lalonde_data[column])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods to analyse the distribution between two groups change according the type of variables and in our case, we will make a distinction between the binary variables (race, married and no degree) and the continuous variables (age, education level and earnings).\n",
    "\n",
    "#### Categorical features (Race, Married and No degree)\n",
    "The goal with categorical variables is to compare the proportions of each feature between the two groups.\n",
    "\n",
    "Compare categorical variables with chi-squared test and continuous variable with two-sample t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_cat=data_control[['black','hispan','married','nodegree']].apply(lambda x:(sum(x)/len(x))*100,axis=0).round(2)\n",
    "treat_cat=data_treat[['black','hispan','married','nodegree']].apply(lambda x:(sum(x)/len(x))*100,axis=0).round(2)\n",
    "data_cat=pd.DataFrame(np.column_stack([control_cat, treat_cat]),index=control_cat.keys(),columns=['Without Job Training (N=429)','With Job Training (N=185)'])\n",
    "data_cat.index.name='Variable'\n",
    "data_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test of independence between the table of contingency for two categorical variables \n",
    "def chisquare_test(var1,var2):\n",
    "    contingency_table=pd.crosstab(var1,var2)\n",
    "    p_value=stats.chi2_contingency(contingency_table)[1]\n",
    "    if p_value<0.05:\n",
    "        test='Significant'\n",
    "    else:\n",
    "        test='Not significant'\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the chi-square test for each categorical feature and add it to the dataframe\n",
    "lalonde_cat=lalonde_data[['black','hispan','married','nodegree']]\n",
    "data_cat['Statistic Test Significance']=lalonde_cat.apply(lambda x: chisquare_test(lalonde_data['treat'],x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized difference for categorical variables\n",
    "def stand_diff_cat(p_c,p_t):\n",
    "    # Convert the percentage in proportions\n",
    "    p_c=p_c/100\n",
    "    p_t=p_t/100\n",
    "    diff=(p_t-p_c)/(np.sqrt((p_t*(1-p_t)+p_c*(1-p_c))/2))\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the standardized difference between for each categorical variable\n",
    "data_cat['Standardized difference']=stand_diff_cat(data_cat['Without Job Training (N=429)'],data_cat['With Job Training (N=185)'])\n",
    "data_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distribution of the different continuous features between the two groups (treat and no treat)\n",
    "fig = plt.figure(1, figsize=(13, 8))\n",
    "plt.subplot(2,2,1)\n",
    "sns.barplot(x=\"treat\",y=\"black\",data=lalonde_data)\n",
    "plt.subplot(2,2,2)\n",
    "sns.barplot(x=\"treat\",y=\"hispan\",data=lalonde_data)\n",
    "plt.subplot(2,2,3)\n",
    "sns.barplot(x=\"treat\",y=\"married\",data=lalonde_data)\n",
    "plt.subplot(2,2,4)\n",
    "sns.barplot(x=\"treat\",y=\"nodegree\",data=lalonde_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous features (Age, Education level and Earnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_cont=data_control[['age','educ','re74','re75','re78']].groupby(lambda idx: 0).agg(['mean','std']).round(2).transpose()\n",
    "treat_cont=data_treat[['age','educ','re74','re75','re78']].groupby(lambda idx: 0).agg(['mean','std']).round(2).transpose()\n",
    "data_cont=pd.concat([control_cont,treat_cont],axis=1)\n",
    "data_cont.columns=['Without Job Training (N=429)','With Job Training (N=185)']\n",
    "data_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two sample t-test\n",
    "def t_test(var1,var2):\n",
    "    p_value=stats.ttest_ind(var1, var2, equal_var=False)[1]\n",
    "    if p_value<0.05:\n",
    "        test='Significant'\n",
    "    else:\n",
    "        test='Not significant'\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalonde_cont=lalonde_data[['age','educ','re74','re75','re78']]\n",
    "# Compute the two sample t-test for each continuous variable\n",
    "t_test_res=lalonde_cont.apply(lambda x: chisquare_test(lalonde_data['treat'],x), axis=0)\n",
    "# Add result to dataframe\n",
    "data_cont.loc[[('age','mean'),('educ','mean'),('re74','mean'),('re75','mean'),('re78','mean')],'Test Significance']=list(zip(t_test_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized difference for continuous variables\n",
    "def stand_diff_cont(x_c,sd_c,x_t,sd_t):\n",
    "    diff=(x_t-x_c)/(np.sqrt((sd_t**2+sd_c**2)/2))\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the standardized difference for each continuous variable and add the result to the dataframe in the same time\n",
    "data_cont.loc[('age','mean'),'Standardized difference']=stand_diff_cont(data_cont.loc[('age','mean'),'Without Job Training (N=429)'],data_cont.loc[('age','std'),'Without Job Training (N=429)'],data_cont.loc[('age','mean'),'With Job Training (N=185)'],data_cont.loc[('age','std'),'With Job Training (N=185)'])\n",
    "data_cont.loc[('educ','mean'),'Standardized difference']=stand_diff_cont(data_cont.loc[('educ','mean'),'Without Job Training (N=429)'],data_cont.loc[('educ','std'),'Without Job Training (N=429)'],data_cont.loc[('educ','mean'),'With Job Training (N=185)'],data_cont.loc[('educ','std'),'With Job Training (N=185)'])\n",
    "data_cont.loc[('re74','mean'),'Standardized difference']=stand_diff_cont(data_cont.loc[('re74','mean'),'Without Job Training (N=429)'],data_cont.loc[('re74','std'),'Without Job Training (N=429)'],data_cont.loc[('re74','mean'),'With Job Training (N=185)'],data_cont.loc[('re74','std'),'With Job Training (N=185)'])\n",
    "data_cont.loc[('re75','mean'),'Standardized difference']=stand_diff_cont(data_cont.loc[('re75','mean'),'Without Job Training (N=429)'],data_cont.loc[('re75','std'),'Without Job Training (N=429)'],data_cont.loc[('re75','mean'),'With Job Training (N=185)'],data_cont.loc[('re75','std'),'With Job Training (N=185)'])\n",
    "data_cont.loc[('re78','mean'),'Standardized difference']=stand_diff_cont(data_cont.loc[('re78','mean'),'Without Job Training (N=429)'],data_cont.loc[('re78','std'),'Without Job Training (N=429)'],data_cont.loc[('re78','mean'),'With Job Training (N=185)'],data_cont.loc[('re78','std'),'With Job Training (N=185)'])\n",
    "data_cont = data_cont.replace(np.nan, '', regex=True)\n",
    "data_cont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distribution of the different continuous features between the two groups (treat and no treat)\n",
    "fig = plt.figure(1, figsize=(17, 10))\n",
    "plt.subplot(2,3,1)\n",
    "sns.boxplot(x='treat', y='age', data=lalonde_data)\n",
    "plt.subplot(2,3,2)\n",
    "sns.boxplot(x='treat', y='educ', data=lalonde_data)\n",
    "plt.subplot(2,3,3)\n",
    "sns.boxplot(x='treat', y='re74', data=lalonde_data)\n",
    "plt.subplot(2,3,4)\n",
    "sns.boxplot(x='treat', y='re75', data=lalonde_data)\n",
    "plt.subplot(2,3,5)\n",
    "sns.boxplot(x='treat', y='re78', data=lalonde_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <font color='purple'>1.3 - A propensity score model</font> </b>\n",
    "<b> Use logistic regression to estimate propensity scores for all points in the dataset.\n",
    "You may use `sklearn` to fit the logistic regression model and apply it to each data point to obtain propensity scores:\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "```\n",
    "\n",
    "Recall that the propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features (in this case, age, education, pre-treatment income, etc.).\n",
    "To brush up on propensity scores, you may read chapter 3.3 of the above-cited book by Rosenbaum or [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).\n",
    "\n",
    "Note: you do not need a train/test split here. Train and apply the model on the entire dataset. If you're wondering why this is the right thing to do in this situation, recall that the propensity score model is not used in order to make predictions about unseen data. Its sole purpose is to balance the dataset across treatment groups.\n",
    "(See p. 74 of Rosenbaum's book for an explanation why slight overfitting is even good for propensity scores.\n",
    "If you want even more information, read [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).) <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression used to estimate the probability of a binary response based on one or more predictor (or independent) variables (features). It allows one to say that the presence of a risk factor increases the odds of a given outcome by a specific factor.\n",
    "\n",
    "\"In propensity-score model, we assumed a linear relationship between continuous covariates and the log-odds of receiving treatment (acces job training program).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates=lalonde_data.drop(['treat'],axis=1)\n",
    "covariates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# Fit the model with X (covariates) and y (dependent variable)\n",
    "X=lalonde_data.drop(['treat'],axis=1)\n",
    "y=lalonde_data.treat\n",
    "logistic = logistic.fit(X, y)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "logistic.score(covariates, lalonde_data.treat)\n",
    "\n",
    "# Probability estimates of the logistic regression that are equal to the propensity scores\n",
    "# Select only the first column that corresponds to the probability of being selected in the treatment group (treat=1)\n",
    "prop_score=logistic.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the propensity score for each point in our dataset\n",
    "lalonde_data['Propensity Score']=prop_score.tolist()\n",
    "lalonde_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the propensity score is balanced across treatment and comparison group\n",
    "data_control=lalonde_data[lalonde_data.treat==0]\n",
    "data_treat=lalonde_data[lalonde_data.treat==1]\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(data_control['Propensity Score'],label=\"Without Job Training Program\",ax=ax)\n",
    "sns.distplot(data_treat['Propensity Score'],label=\"With Job Training Program\",ax=ax)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Propensity score')\n",
    "ax.set_ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <font color='purple'>1.4 - Balancing the dataset via matching</font> </b>\n",
    "<b> Use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group.\n",
    "(Hint: you may explore the `networkx` package in Python for predefined matching functions.)\n",
    "\n",
    "Your matching should maximize the similarity between matched subjects, as captured by their propensity scores.\n",
    "In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "After matching, you have as many treated as you have control subjects.\n",
    "Compare the outcomes (`re78`) between the two groups (treated and control).\n",
    "\n",
    "Also, compare again the feature-value distributions between the two groups, as you've done in part 2 above, but now only for the matched subjects.\n",
    "What do you observe?\n",
    "Are you closer to being able to draw valid conclusions now than you were before? <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute a matching between the two groups, we are going to create a graph from the data, and then use the max_weight_matching function from the networkx package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create our graph from the Lalonde data. Each node will have the attributes score containing its propensity score, and bipartite representing the categorical attribute 'treat'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for d in range(len(lalonde_data)):\n",
    "    G.add_node(lalonde_data.index[d], score=lalonde_data.iloc[d]['Propensity Score'], bipartite=int(lalonde_data.iloc[d]['treat']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide our nodes into two sets : those from the control group, and those from the treated group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_nodes = list(n for n in G if G.node[n]['bipartite']==0)\n",
    "treat_nodes = G.nodes() - control_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add an edge from every treat node to every control node, with a weight representing their propensity score difference. Since we want to get the maximum similarity between two subjects, we want to construct pairs of subjects such that the difference between their propensity score is minimal. The matching function we are going to use match two nodes if the weight of the edge connecting them is maximal. In order to have a maximum weight if two subjects are similar, we define this weight to be the absolute value of the difference of their score substracted to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in treat_nodes:\n",
    "    for c in control_nodes:\n",
    "        edge_weight = 1 - abs(G.node[t]['score'] - G.node[c]['score'])\n",
    "        G.add_edge(t,c, weight = edge_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the max_weight_matching function of networkx to match each data point from the treated group with exactly one data point from the control group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "#matching = nx.max_weight_matching(G, maxcardinality=True)\n",
    "#pickle.dump( matching, open( \"matching.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "matching = pickle.load(open( \"matching.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our resulting matching dictionary contains the edges from the treated group to the control group, but also the reversed edges from the control group to the treated group. So we create a dataframe from our dictionary and keep only the first half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matching = (pd.DataFrame.from_records([matching]).transpose())[0].reset_index()\n",
    "first_half = int(len(matching)/2)\n",
    "print(\"We check that the length of the dictionary is the two times the size of the treated group :\")\n",
    "print(\"size of dictionary / 2 = \" + str(first_half)\n",
    "        + \" \\nsize of treated group = \" + str(len(data_treat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matching = df_matching[:first_half].copy()\n",
    "df_matching.columns=['Treated group', 'Control group']\n",
    "df_matching.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the earnings between the two resulting groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_control_subset = data_control[data_control.index.isin(df_matching['Control group'])]\n",
    "# Visualization of the respective distribution of the outcome variable for the two groups with an histogram \n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "# Histrogram for the control group\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(data_control.re78,color='#00ccff')\n",
    "plt.title('Earnings distribution for the control group')\n",
    "plt.xlabel('real earning in 1978 [$]')\n",
    "plt.ylabel('frequency')\n",
    "# Histrogram for the control group\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(data_control_subset.re78,color='#00ccff')\n",
    "plt.title('Earnings distribution for the control group subset')\n",
    "plt.xlabel('real earning in 1978 [$]')\n",
    "plt.ylabel('frequency')\n",
    "# Histrogram for the treated group\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(data_treat.re78,color='#00ccff')\n",
    "plt.title('Earnings distribution of earning for the treated group')\n",
    "plt.xlabel('real earning in 1978 [$]')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_cat_subset=data_control_subset[['black','hispan','married','nodegree']].apply(lambda x:(sum(x)/len(x))*100,axis=0).round(2)\n",
    "data_cat_subset=pd.DataFrame(np.column_stack([control_cat_subset, treat_cat]),index=control_cat_subset.keys(),columns=['Without Job Training (N=185)','With Job Training (N=185)'])\n",
    "data_cat_subset.index.name='Variable'\n",
    "data_cat_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the unmatched subjects in the control group\n",
    "lalonde_subset = lalonde_data[(lalonde_data.index.isin(df_matching['Control group'])) | (lalonde_data.index.isin(df_matching['Treated group']))]\n",
    "\n",
    "# Compute the chi-square test for each categorical feature and add it to the dataframe\n",
    "lalonde_cat_subset=lalonde_subset[['black','hispan','married','nodegree']]\n",
    "data_cat_subset['Statistic Test Significance']=lalonde_cat_subset.apply(lambda x: chisquare_test(lalonde_subset['treat'],x), axis=0)\n",
    "\n",
    "# Compute the standardized difference between for each categorical variable\n",
    "data_cat_subset['Standardized difference']=stand_diff_cat(data_cat_subset['Without Job Training (N=185)'],data_cat_subset['With Job Training (N=185)'])\n",
    "data_cat_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distribution of the different continuous features between the two groups (treat and control)\n",
    "fig = plt.figure(1, figsize=(13, 8))\n",
    "plt.subplot(2,2,1)\n",
    "sns.barplot(x=\"treat\",y=\"black\",data=lalonde_subset)\n",
    "plt.subplot(2,2,2)\n",
    "sns.barplot(x=\"treat\",y=\"hispan\",data=lalonde_subset)\n",
    "plt.subplot(2,2,3)\n",
    "sns.barplot(x=\"treat\",y=\"married\",data=lalonde_subset)\n",
    "plt.subplot(2,2,4)\n",
    "sns.barplot(x=\"treat\",y=\"nodegree\",data=lalonde_subset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_cont_subset=data_control_subset[['age','educ','re74','re75','re78']].groupby(lambda idx: 0).agg(['mean','std']).round(2).transpose()\n",
    "data_cont_subset=pd.concat([control_cont_subset,treat_cont],axis=1)\n",
    "data_cont_subset.columns=['Without Job Training (N=185)','With Job Training (N=185)']\n",
    "\n",
    "lalonde_cont_subset=lalonde_subset[['age','educ','re74','re75','re78']]\n",
    "# Compute the two samples t-test for each continuous variable\n",
    "t_test_res_subset=lalonde_cont_subset.apply(lambda x: chisquare_test(lalonde_subset['treat'],x), axis=0)\n",
    "# Add result to dataframe\n",
    "data_cont_subset.loc[[('age','mean'),('educ','mean'),('re74','mean'),('re75','mean'),('re78','mean')],'Test Significance']=list(zip(t_test_res_subset))\n",
    "\n",
    "# Compute the standardized difference for each continuous variable and add the result to the dataframe in the same time\n",
    "data_cont_subset.loc[('age', 'mean'),'Standardized difference']=stand_diff_cont(data_cont_subset.loc[('age', 'mean'),'Without Job Training (N=185)'],data_cont_subset.loc[('age', 'std'),'Without Job Training (N=185)'],data_cont_subset.loc[('age','mean'),'With Job Training (N=185)'],data_cont_subset.loc[('age','std'),'With Job Training (N=185)'])\n",
    "data_cont_subset.loc[('educ','mean'),'Standardized difference']=stand_diff_cont(data_cont_subset.loc[('educ','mean'),'Without Job Training (N=185)'],data_cont_subset.loc[('educ','std'),'Without Job Training (N=185)'],data_cont_subset.loc[('educ','mean'),'With Job Training (N=185)'],data_cont_subset.loc[('educ','std'),'With Job Training (N=185)'])\n",
    "data_cont_subset.loc[('re74','mean'),'Standardized difference']=stand_diff_cont(data_cont_subset.loc[('re74','mean'),'Without Job Training (N=185)'],data_cont_subset.loc[('re74','std'),'Without Job Training (N=185)'],data_cont_subset.loc[('re74','mean'),'With Job Training (N=185)'],data_cont_subset.loc[('re74','std'),'With Job Training (N=185)'])\n",
    "data_cont_subset.loc[('re75','mean'),'Standardized difference']=stand_diff_cont(data_cont_subset.loc[('re75','mean'),'Without Job Training (N=185)'],data_cont_subset.loc[('re75','std'),'Without Job Training (N=185)'],data_cont_subset.loc[('re75','mean'),'With Job Training (N=185)'],data_cont_subset.loc[('re75','std'),'With Job Training (N=185)'])\n",
    "data_cont_subset.loc[('re78','mean'),'Standardized difference']=stand_diff_cont(data_cont_subset.loc[('re78','mean'),'Without Job Training (N=185)'],data_cont_subset.loc[('re78','std'),'Without Job Training (N=185)'],data_cont_subset.loc[('re78','mean'),'With Job Training (N=185)'],data_cont_subset.loc[('re78','std'),'With Job Training (N=185)'])\n",
    "data_cont_subset = data_cont_subset.replace(np.nan, '', regex=True)\n",
    "data_cont_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distribution of the different continuous features between the two groups (treat and control)\n",
    "fig = plt.figure(1, figsize=(17, 10))\n",
    "plt.subplot(2,3,1)\n",
    "sns.boxplot(x='treat', y='age', data=lalonde_subset)\n",
    "plt.subplot(2,3,2)\n",
    "sns.boxplot(x='treat', y='educ', data=lalonde_subset)\n",
    "plt.subplot(2,3,3)\n",
    "sns.boxplot(x='treat', y='re74', data=lalonde_subset)\n",
    "plt.subplot(2,3,4)\n",
    "sns.boxplot(x='treat', y='re75', data=lalonde_subset)\n",
    "plt.subplot(2,3,5)\n",
    "sns.boxplot(x='treat', y='re78', data=lalonde_subset)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the propensity score is balanced across treatment and comparison group\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(data_control_subset['Propensity Score'],label=\"Without Job Training Program\",ax=ax)\n",
    "sns.distplot(data_treat['Propensity Score'],label=\"With Job Training Program\",ax=ax)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Propensity score')\n",
    "ax.set_ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Therefore, the appropriateness of the specification of the propensity score is assessed by examining the degree to which matching on the estimated propensity score has resulted in a matched sample in which the distribution of measured baseline covariates is similar between treated and untreated subjects (removed observed systematic differences between treated and untreated subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <font color='purple'>1.5 - Balancing the groups further</font> </b>\n",
    "<b> Based on your comparison of feature-value distributions from part 4, are you fully satisfied with your matching?\n",
    "Would you say your dataset is sufficiently balanced?\n",
    "If not, in what ways could the \"balanced\" dataset you have obtained still not allow you to draw valid conclusions?\n",
    "\n",
    "Improve your matching by explicitly making sure that you match only subjects that have the same value for the problematic feature.\n",
    "Argue with numbers and plots that the two groups (treated and control) are now better balanced than after part 4. <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe on the last plot that the propensity score is still not enough balanced between the two groups. From the $\\chi$-squared test on the categorical attributes, we note that the skin color is a significant variable. In order to extract valid conclusions, we need to take into account the skin color of the subjects during the matching. Therefore, we will proceed as in the previous question, but this time only connecting nodes if they have the same skin color in the original graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "for d in range(len(lalonde_data)):\n",
    "    G.add_node(lalonde_data.index[d], score=lalonde_data.iloc[d]['Propensity Score'], black=int(lalonde_data.iloc[d]['black']), bipartite=int(lalonde_data.iloc[d]['treat']))\n",
    "\n",
    "# Divide the data into 4 subsets based on the treat and the black attribute\n",
    "control_nodes_black = list(n for n in G if (G.node[n]['bipartite']==0)&(G.node[n]['black']==1))\n",
    "treat_nodes_black = list(n for n in G if (G.node[n]['bipartite']==1)&(G.node[n]['black']==1))\n",
    "control_nodes_white = list(n for n in G if (G.node[n]['bipartite']==0)&(G.node[n]['black']==0))\n",
    "treat_nodes_white = list(n for n in G if (G.node[n]['bipartite']==1)&(G.node[n]['black']==0))\n",
    "\n",
    "               \n",
    "# Add edges between each group into the 'black subset'\n",
    "for t in treat_nodes:\n",
    "    for c in control_nodes_black:\n",
    "        edge_weight = 1 - abs(G.node[t]['score'] - G.node[c]['score'])\n",
    "        G.add_edge(t,c, weight = edge_weight)\n",
    "               \n",
    "# Add edges between each group into the 'white subset'\n",
    "for t in treat_nodes_white:\n",
    "    for c in control_nodes_white:\n",
    "        edge_weight = 1 - abs(G.node[t]['score'] - G.node[c]['score'])\n",
    "        G.add_edge(t,c, weight = edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_matching = nx.max_weight_matching(G, maxcardinality=True)\n",
    "#pickle.dump( new_matching, open( \"matching2.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matching = pickle.load(open(\"matching2.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_matching = (pd.DataFrame.from_records([new_matching]).transpose())[0].reset_index()\n",
    "first_half = int(len(new_matching)/2)\n",
    "df_new_matching = df_new_matching[:first_half].copy()\n",
    "df_new_matching.columns=['Treated group', 'Control group']\n",
    "print(\"Number of subjects from each group in the resulting matching : \" + str(len(df_new_matching)))\n",
    "df_new_matching.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is less black people in the control group than in the treated group, our matching contains less subjects, as not all nodes in the treated set can match one control node. We obtain a dataframe matching 116 subjects from the treated group to 116 subjects from the control group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the repartition for the two new groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_control_subset2 = data_control[data_control.index.isin(df_new_matching['Control group'])]\n",
    "control_cat_subset2=data_control_subset2[['black','hispan','married','nodegree']].apply(lambda x : (sum(x)/len(x))*100,axis=0).round(2)\n",
    "data_treat_subset2 = data_treat[data_treat.index.isin(df_new_matching['Treated group'])]\n",
    "treat_cat_subset2 = data_treat_subset2[['black', 'hispan', 'married','nodegree']].apply(lambda x : (sum(x)/len(x))*100,axis=0).round(2)\n",
    "data_cat_subset2=pd.DataFrame(np.column_stack([control_cat_subset2, treat_cat_subset2]),index=control_cat_subset2.keys(),columns=['Without Job Training (N=116)','With Job Training (N=116)'])\n",
    "data_cat_subset2.index.name='Variable'\n",
    "data_cat_subset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the unmatched subjects in the control group\n",
    "lalonde_subset2 = lalonde_data[(lalonde_data.index.isin(df_new_matching['Control group'])) | (lalonde_data.index.isin(df_new_matching['Treated group']))]\n",
    "\n",
    "# Compute the chi-square test for each categorical feature and add it to the dataframe\n",
    "lalonde_cat_subset2=lalonde_subset2[['black','hispan','married','nodegree']]\n",
    "data_cat_subset2['Statistic Test Significance']=lalonde_cat_subset2.apply(lambda x: chisquare_test(lalonde_subset2['treat'],x), axis=0)\n",
    "\n",
    "# Compute the standardized difference between for each categorical variable\n",
    "data_cat_subset2['Standardized difference']=stand_diff_cat(data_cat_subset2['Without Job Training (N=116)'],data_cat_subset2['With Job Training (N=116)'])\n",
    "data_cat_subset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distribution of the different continuous features between the two groups (treat and control)\n",
    "fig = plt.figure(1, figsize=(13, 8))\n",
    "plt.subplot(2,2,1)\n",
    "sns.barplot(x=\"treat\",y=\"black\",data=lalonde_subset2)\n",
    "plt.subplot(2,2,2)\n",
    "sns.barplot(x=\"treat\",y=\"hispan\",data=lalonde_subset2)\n",
    "plt.subplot(2,2,3)\n",
    "sns.barplot(x=\"treat\",y=\"married\",data=lalonde_subset2)\n",
    "plt.subplot(2,2,4)\n",
    "sns.barplot(x=\"treat\",y=\"nodegree\",data=lalonde_subset2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the propensity score is balanced across treatment and comparison group\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(data_control_subset2['Propensity Score'],label=\"Without Job Training Program\",ax=ax)\n",
    "sns.distplot(data_treat_subset2['Propensity Score'],label=\"With Job Training Program\",ax=ax)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Propensity score')\n",
    "ax.set_ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots and the $\\chi$-squared test, we can assert that the two groups are better balanced with the matching considering the skin color of the subjects than with the previous matching only based on the propensity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <font color='purple'>1.6 - A less naive analysis</font> </b>\n",
    "<b> Compare the outcomes (`re78`) between treated and control subjects, as you've done in part 1, but now only for the matched dataset you've obtained from part 5.\n",
    "What do you conclude about the effectiveness of the job training program? <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous questions, we first compare the outcomes by displaying a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the respective distribution of the outcome variable for the two groups with an histogram \n",
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "# Histrogram for the treated group\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(data_treat_subset2.re78,color='#00ccff')\n",
    "plt.title('Earnings distribution of earning for the treated group in the new matching')\n",
    "plt.xlabel('real earning in 1978 [$]')\n",
    "plt.ylabel('frequency')\n",
    "# Histrogram for the control group in the new matching\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(data_control_subset2.re78,color='#00ccff')\n",
    "plt.title('Earnings distribution for the control group in the new matching')\n",
    "plt.xlabel('real earning in 1978 [$]')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the density of outcomes\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(data_control_subset2['re78'],label=\"Without Job Training Program\",ax=ax)\n",
    "sns.distplot(data_treat_subset2['re78'],label=\"With Job Training Program\",ax=ax)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Outcomes')\n",
    "ax.set_ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalonde_subset2[['treat','re78']].groupby(['treat']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the values of earnings in 1978 into intervals\n",
    "int_re78 = pd.cut(lalonde_subset2.re78, 8)\n",
    "\n",
    "earnings_intervals = lalonde_subset2['treat'].to_frame()\n",
    "earnings_intervals['control'] = 1-earnings_intervals['treat']\n",
    "\n",
    "earnings_intervals['Earnings intervals'] = (int_re78.to_frame())['re78']\n",
    "earnings_intervals = earnings_intervals.groupby('Earnings intervals').sum()\n",
    "earnings_intervals['treat %'] = earnings_intervals['treat']*100/len(lalonde_subset2)\n",
    "earnings_intervals ['control %'] = earnings_intervals['control']*100/len(lalonde_subset2)\n",
    "earnings_intervals['treat % inter'] = earnings_intervals['treat']*100/(earnings_intervals['treat']+earnings_intervals['control'])\n",
    "earnings_intervals ['control % inter'] = 100 - earnings_intervals['treat % inter']\n",
    "earnings_intervals.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize=(7,5))\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "sns.barplot(x=\"treat\",y=\"re78\", data=lalonde_subset2, ax=ax1, alpha=0.85,errcolor='.5')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Control and treated groups')\n",
    "ax1.set_ylabel('Earnings in 1978 ($)')\n",
    "ax1.set_title('Outcomes among the two groups', fontsize=14)\n",
    "\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "earnings_intervals[['treat','control']].plot(kind=\"bar\",alpha=0.6, ax=ax2)\n",
    "ax2.set_ylabel(\"Number of subjects\")\n",
    "ax2.set_title(\"Comparison of earnings between the two groups\", fontsize=15)\n",
    "ax2.legend(loc=[0.85,0.825])\n",
    "\n",
    "ax3 = plt.subplot(2,2,4)\n",
    "earnings_intervals[['treat % inter','control % inter']].plot(kind=\"bar\",alpha=0.6, ax=ax3)\n",
    "ax3.set_ylabel(\"Subjects in each interval (%)\")\n",
    "ax3.set_title(\"Comparison of earnings between the two groups for each interval(%)\", fontsize=15)\n",
    "ax3.legend(loc=[0.01,0.80])\n",
    "\n",
    "plt.tight_layout(pad=1.5, w_pad=2.0, h_pad=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots, we see that in average the treated group get higher earnings than the control group. If we look at the number of people in each interval of earnings, we notice that some outliers in the treated group earned more than 20 000 $ in 1978. We could say that people with job training earn globally more than those without job training, but the results are not outstanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> <font color='purple'>Question 2 - Applied ML</font> </b>\n",
    "<b>We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project if you plan to work with text!<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <font color='purple'>2.1</font> </b>\n",
    "<b> Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn ([link](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)).  \n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequencyâinverse document frequency, is of great help when if comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category). <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <font color='purple'>2.2</font> </b>\n",
    "<b> Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results.\n",
    " <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
